<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../../img/favicon.ico" />
    <title>Index - Sparse Autoencoder</title>
    <link rel="stylesheet" href="../../css/theme.css" />
    <link rel="stylesheet" href="../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="../../assets/_mkdocstrings.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Index";
        var mkdocs_page_input_path = "reference/activation_resampler/index.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../.." class="icon icon-home"> Sparse Autoencoder
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../..">Home</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../../getting_started/">Getting Started</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../../contributing/">Contributing</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../../citation/">Citation</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Reference</span></p>
              <ul class="current">
                  <li class="toctree-l1"><a class="reference internal" href="../">Home</a>
                  </li>
                  <li class="toctree-l1 current"><a class="reference internal current" href="#">activation_resampler</a>
    <ul class="current">
                <li class="toctree-l2 current"><a class="reference internal current" href="./">Index</a>
    <ul class="current">
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="abstract_activation_resampler/">abstract_activation_resampler</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="activation_resampler/">activation_resampler</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">activation_store</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../activation_store/">Index</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../activation_store/base_store/">base_store</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../activation_store/disk_store/">disk_store</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../activation_store/list_store/">list_store</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../activation_store/tensor_store/">tensor_store</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="#">utils</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../activation_store/utils/">Index</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../activation_store/utils/extend_resize/">extend_resize</a>
                </li>
    </ul>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">autoencoder</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../autoencoder/">Index</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../autoencoder/abstract_autoencoder/">abstract_autoencoder</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="#">components</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../autoencoder/components/">Index</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../autoencoder/components/abstract_decoder/">abstract_decoder</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../autoencoder/components/abstract_encoder/">abstract_encoder</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../autoencoder/components/abstract_outer_bias/">abstract_outer_bias</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../autoencoder/components/linear_encoder/">linear_encoder</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../autoencoder/components/tied_bias/">tied_bias</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../autoencoder/components/unit_norm_decoder/">unit_norm_decoder</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../autoencoder/components/unit_norm_linear/">unit_norm_linear</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../autoencoder/fista_autoencoder/">fista_autoencoder</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../autoencoder/loss/">loss</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../autoencoder/model/">model</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">loss</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../loss/">Index</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../loss/abstract_loss/">abstract_loss</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../loss/fista/">fista</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../loss/learned_activations_l1/">learned_activations_l1</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../loss/mse_reconstruction_loss/">mse_reconstruction_loss</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../loss/reducer/">reducer</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">metrics</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../metrics/">Index</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../metrics/abstract_metric/">abstract_metric</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="#">generate</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../metrics/generate/">Index</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../metrics/generate/abstract_generate_metric/">abstract_generate_metric</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="#">train</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../metrics/train/">Index</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../metrics/train/abstract_train_metric/">abstract_train_metric</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="#">validate</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../metrics/validate/">Index</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../metrics/validate/abstract_validate_metric/">abstract_validate_metric</a>
                </li>
    </ul>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">optimizer</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../optimizer/">Index</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../optimizer/abstract_optimizer/">abstract_optimizer</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../optimizer/adam_with_reset/">adam_with_reset</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">source_data</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../source_data/">Index</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../source_data/abstract_dataset/">abstract_dataset</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../source_data/c4_pre_tokenized/">c4_pre_tokenized</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../source_data/pile_uncopyrighted/">pile_uncopyrighted</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../source_data/pretokenized_dataset/">pretokenized_dataset</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../source_data/random_int/">random_int</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../source_data/text_dataset/">text_dataset</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">source_model</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../source_model/">Index</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../source_model/store_activations_hook/">store_activations_hook</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">src_model</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../src_model/">Index</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../src_model/store_activations_hook/">store_activations_hook</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tensor_types/">tensor_types</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">train</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../train/">Index</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../train/abstract_pipeline/">abstract_pipeline</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../train/generate_activations/">generate_activations</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="#">metrics</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../train/metrics/">Index</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../train/metrics/capacity/">capacity</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../train/metrics/feature_density/">feature_density</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../train/pipeline/">pipeline</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../train/resample_neurons/">resample_neurons</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../train/sweep_config/">sweep_config</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../train/train_autoencoder/">train_autoencoder</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="#">utils</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../train/utils/">Index</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../train/utils/get_model_device/">get_model_device</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../train/utils/wandb_sweep_types/">wandb_sweep_types</a>
                </li>
    </ul>
                </li>
    </ul>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../..">Sparse Autoencoder</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../.." class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">Reference</li>
          <li class="breadcrumb-item">activation_resampler</li>
      <li class="breadcrumb-item active">Index</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <div class="doc doc-object doc-module">



<a id="sparse_autoencoder.activation_resampler"></a>
  <div class="doc doc-contents first">
  
      <p>Activation Resampler.</p>

  

  <div class="doc doc-children">








<div class="doc doc-object doc-class">




<h2 id="sparse_autoencoder.activation_resampler.AbstractActivationResampler" class="doc doc-heading">
          <code>AbstractActivationResampler</code>


</h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><span title="abc.ABC">ABC</span></code></p>

  
      <p>Abstract activation resampler.</p>

            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/activation_resampler/abstract_activation_resampler.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">AbstractActivationResampler</span><span class="p">(</span><span class="n">ABC</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Abstract activation resampler.&quot;&quot;&quot;</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">resample_dead_neurons</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">neuron_activity</span><span class="p">:</span> <span class="n">NeuronActivity</span><span class="p">,</span>
        <span class="n">activation_store</span><span class="p">:</span> <span class="n">TensorActivationStore</span><span class="p">,</span>
        <span class="n">autoencoder</span><span class="p">:</span> <span class="n">SparseAutoencoder</span><span class="p">,</span>
        <span class="n">loss_fn</span><span class="p">:</span> <span class="n">AbstractLoss</span><span class="p">,</span>
        <span class="n">train_batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_inputs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">819_200</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ParameterUpdateResults</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Resample dead neurons.</span>

<span class="sd">        Args:</span>
<span class="sd">            neuron_activity: Number of times each neuron fired.</span>
<span class="sd">            activation_store: Activation store.</span>
<span class="sd">            autoencoder: Sparse autoencoder model.</span>
<span class="sd">            loss_fn: Loss function.</span>
<span class="sd">            train_batch_size: Train batch size (also used for resampling).</span>
<span class="sd">            num_inputs: Number of input activations to use when resampling. Will be rounded down to</span>
<span class="sd">                be divisible by the batch size, and cannot be larger than the number of items</span>
<span class="sd">                currently in the store.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.activation_resampler.AbstractActivationResampler.resample_dead_neurons" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">resample_dead_neurons</span><span class="p">(</span><span class="n">neuron_activity</span><span class="p">,</span> <span class="n">activation_store</span><span class="p">,</span> <span class="n">autoencoder</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">train_batch_size</span><span class="p">,</span> <span class="n">num_inputs</span><span class="o">=</span><span class="mi">819200</span><span class="p">)</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-abstractmethod"><code>abstractmethod</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>Resample dead neurons.</p>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>neuron_activity</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.NeuronActivity" href="../tensor_types/#sparse_autoencoder.tensor_types.NeuronActivity">NeuronActivity</a></code>)
              –
              <div class="doc-md-description">
                <p>Number of times each neuron fired.</p>
              </div>
            </li>
            <li>
              <b><code>activation_store</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.activation_store.tensor_store.TensorActivationStore" href="../activation_store/tensor_store/#sparse_autoencoder.activation_store.tensor_store.TensorActivationStore">TensorActivationStore</a></code>)
              –
              <div class="doc-md-description">
                <p>Activation store.</p>
              </div>
            </li>
            <li>
              <b><code>autoencoder</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.autoencoder.model.SparseAutoencoder" href="../autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder">SparseAutoencoder</a></code>)
              –
              <div class="doc-md-description">
                <p>Sparse autoencoder model.</p>
              </div>
            </li>
            <li>
              <b><code>loss_fn</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.loss.abstract_loss.AbstractLoss" href="../loss/abstract_loss/#sparse_autoencoder.loss.abstract_loss.AbstractLoss">AbstractLoss</a></code>)
              –
              <div class="doc-md-description">
                <p>Loss function.</p>
              </div>
            </li>
            <li>
              <b><code>train_batch_size</code></b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>Train batch size (also used for resampling).</p>
              </div>
            </li>
            <li>
              <b><code>num_inputs</code></b>
                  (<code>int</code>, default:
                      <code>819200</code>
)
              –
              <div class="doc-md-description">
                <p>Number of input activations to use when resampling. Will be rounded down to
be divisible by the batch size, and cannot be larger than the number of items
currently in the store.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary> <code>sparse_autoencoder/activation_resampler/abstract_activation_resampler.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@abstractmethod</span>
<span class="k">def</span> <span class="nf">resample_dead_neurons</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">neuron_activity</span><span class="p">:</span> <span class="n">NeuronActivity</span><span class="p">,</span>
    <span class="n">activation_store</span><span class="p">:</span> <span class="n">TensorActivationStore</span><span class="p">,</span>
    <span class="n">autoencoder</span><span class="p">:</span> <span class="n">SparseAutoencoder</span><span class="p">,</span>
    <span class="n">loss_fn</span><span class="p">:</span> <span class="n">AbstractLoss</span><span class="p">,</span>
    <span class="n">train_batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">num_inputs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">819_200</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ParameterUpdateResults</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Resample dead neurons.</span>

<span class="sd">    Args:</span>
<span class="sd">        neuron_activity: Number of times each neuron fired.</span>
<span class="sd">        activation_store: Activation store.</span>
<span class="sd">        autoencoder: Sparse autoencoder model.</span>
<span class="sd">        loss_fn: Loss function.</span>
<span class="sd">        train_batch_size: Train batch size (also used for resampling).</span>
<span class="sd">        num_inputs: Number of input activations to use when resampling. Will be rounded down to</span>
<span class="sd">            be divisible by the batch size, and cannot be larger than the number of items</span>
<span class="sd">            currently in the store.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">




<h2 id="sparse_autoencoder.activation_resampler.ActivationResampler" class="doc doc-heading">
          <code>ActivationResampler</code>


</h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.activation_resampler.abstract_activation_resampler.AbstractActivationResampler" href="abstract_activation_resampler/#sparse_autoencoder.activation_resampler.abstract_activation_resampler.AbstractActivationResampler">AbstractActivationResampler</a></code></p>

  
      <p>Activation resampler.</p>
<p>Over the course of training, a subset of autoencoder neurons will have zero activity across
a large number of datapoints. The authors of <em>Towards Monosemanticity: Decomposing Language
Models With Dictionary Learning</em> found that “resampling” these dead neurons during training
improves the number of likely-interpretable features (i.e., those in the high density cluster)
and reduces total loss. This resampling may be compatible with the Lottery Ticket Hypothesis and
increase the number of chances the network has to find promising feature directions.</p>
<p>An interesting nuance around dead neurons involves the ultralow density cluster. They found that
if we increase the number of training steps then networks will kill off more of these ultralow
density neurons. This reinforces the use of the high density cluster as a useful metric because
there can exist neurons that are de facto dead but will not appear to be when looking at the
number of dead neurons alone.</p>
<p>This approach is designed to seed new features to fit inputs where the current autoencoder
performs worst. Resetting the encoder norm and bias are crucial to ensuring this resampled
neuron will only fire weakly for inputs similar to the one used for its reinitialization. This
was done to minimize interference with the rest of the network.</p>

<details class="warning" open>
  <summary>Warning</summary>
  <p>The optimizer should be reset after applying this function, as the Adam state will be
incorrect for the modified weights and biases.</p>
<p>Note this approach is also known to create sudden loss spikes, and resampling too frequently
causes training to diverge.</p>
</details>
            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">ActivationResampler</span><span class="p">(</span><span class="n">AbstractActivationResampler</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Activation resampler.</span>

<span class="sd">    Over the course of training, a subset of autoencoder neurons will have zero activity across</span>
<span class="sd">    a large number of datapoints. The authors of *Towards Monosemanticity: Decomposing Language</span>
<span class="sd">    Models With Dictionary Learning* found that “resampling” these dead neurons during training</span>
<span class="sd">    improves the number of likely-interpretable features (i.e., those in the high density cluster)</span>
<span class="sd">    and reduces total loss. This resampling may be compatible with the Lottery Ticket Hypothesis and</span>
<span class="sd">    increase the number of chances the network has to find promising feature directions.</span>

<span class="sd">    An interesting nuance around dead neurons involves the ultralow density cluster. They found that</span>
<span class="sd">    if we increase the number of training steps then networks will kill off more of these ultralow</span>
<span class="sd">    density neurons. This reinforces the use of the high density cluster as a useful metric because</span>
<span class="sd">    there can exist neurons that are de facto dead but will not appear to be when looking at the</span>
<span class="sd">    number of dead neurons alone.</span>

<span class="sd">    This approach is designed to seed new features to fit inputs where the current autoencoder</span>
<span class="sd">    performs worst. Resetting the encoder norm and bias are crucial to ensuring this resampled</span>
<span class="sd">    neuron will only fire weakly for inputs similar to the one used for its reinitialization. This</span>
<span class="sd">    was done to minimize interference with the rest of the network.</span>

<span class="sd">    Warning:</span>
<span class="sd">        The optimizer should be reset after applying this function, as the Adam state will be</span>
<span class="sd">        incorrect for the modified weights and biases.</span>

<span class="sd">        Note this approach is also known to create sudden loss spikes, and resampling too frequently</span>
<span class="sd">        causes training to diverge.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">get_dead_neuron_indices</span><span class="p">(</span>
        <span class="n">neuron_activity</span><span class="p">:</span> <span class="n">NeuronActivity</span><span class="p">,</span> <span class="n">threshold</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">LearntNeuronIndices</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Identify the indices of neurons that have zero activity.</span>

<span class="sd">        Example:</span>
<span class="sd">            &gt;&gt;&gt; neuron_activity = torch.tensor([0, 0, 3, 10, 0])</span>
<span class="sd">            &gt;&gt;&gt; dead_neuron_indices = ActivationResampler.get_dead_neuron_indices(neuron_activity)</span>
<span class="sd">            &gt;&gt;&gt; dead_neuron_indices.tolist()</span>
<span class="sd">            [0, 1, 4]</span>

<span class="sd">        Args:</span>
<span class="sd">            neuron_activity: Tensor representing the number of times each neuron fired.</span>
<span class="sd">            threshold: Threshold for determining if a neuron is dead (has fired less than this</span>
<span class="sd">                number of times.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A tensor containing the indices of neurons that are &#39;dead&#39; (zero activity).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">neuron_activity</span> <span class="o">&lt;=</span> <span class="n">threshold</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">compute_loss_and_get_activations</span><span class="p">(</span>
        <span class="n">store</span><span class="p">:</span> <span class="n">ActivationStore</span><span class="p">,</span>
        <span class="n">autoencoder</span><span class="p">:</span> <span class="n">SparseAutoencoder</span><span class="p">,</span>
        <span class="n">loss_fn</span><span class="p">:</span> <span class="n">AbstractLoss</span><span class="p">,</span>
        <span class="n">num_inputs</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">train_batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">TrainBatchStatistic</span><span class="p">,</span> <span class="n">InputOutputActivationBatch</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Compute the loss on a random subset of inputs.</span>

<span class="sd">        Computes the loss and also stores the input activations (for use in resampling neurons).</span>

<span class="sd">        Args:</span>
<span class="sd">            store: Activation store.</span>
<span class="sd">            autoencoder: Sparse autoencoder model.</span>
<span class="sd">            loss_fn: Loss function.</span>
<span class="sd">            num_inputs: Number of input activations to use.</span>
<span class="sd">            train_batch_size: Train batch size (also used for resampling).</span>

<span class="sd">        Returns:</span>
<span class="sd">            A tuple containing the loss per item, and all input activations.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">loss_batches</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">TrainBatchStatistic</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">input_activations_batches</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">InputOutputActivationBatch</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">store</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">train_batch_size</span><span class="p">)</span>
            <span class="n">batches</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">num_inputs</span> <span class="o">//</span> <span class="n">train_batch_size</span>

            <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">dataloader</span><span class="p">)):</span>
                <span class="n">input_activations_batches</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
                <span class="n">learned_activations</span><span class="p">,</span> <span class="n">reconstructed_activations</span> <span class="o">=</span> <span class="n">autoencoder</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
                <span class="n">loss_batches</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                    <span class="n">loss_fn</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">learned_activations</span><span class="p">,</span> <span class="n">reconstructed_activations</span><span class="p">)</span>
                <span class="p">)</span>
                <span class="k">if</span> <span class="n">batch_idx</span> <span class="o">&gt;=</span> <span class="n">batches</span><span class="p">:</span>
                    <span class="k">break</span>

            <span class="n">loss_result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">loss_batches</span><span class="p">)</span>
            <span class="n">input_activations</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">input_activations_batches</span><span class="p">)</span>

            <span class="c1"># Check we generated enough data</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">loss_result</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">num_inputs</span><span class="p">:</span>
                <span class="n">error_message</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Cannot get </span><span class="si">{</span><span class="n">num_inputs</span><span class="si">}</span><span class="s2"> items from the store, &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;as only </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">loss_result</span><span class="p">)</span><span class="si">}</span><span class="s2"> were available.&quot;</span>
                <span class="p">)</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">error_message</span><span class="p">)</span>

            <span class="k">return</span> <span class="n">loss_result</span><span class="p">,</span> <span class="n">input_activations</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">assign_sampling_probabilities</span><span class="p">(</span><span class="n">loss</span><span class="p">:</span> <span class="n">TrainBatchStatistic</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Assign the sampling probabilities for each input activations vector.</span>

<span class="sd">        Assign each input vector a probability of being picked that is proportional to the square of</span>
<span class="sd">        the autoencoder&#39;s loss on that input.</span>

<span class="sd">        Example:</span>
<span class="sd">            &gt;&gt;&gt; loss = torch.tensor([1.0, 2.0, 3.0])</span>
<span class="sd">            &gt;&gt;&gt; ActivationResampler.assign_sampling_probabilities(loss).round(decimals=1)</span>
<span class="sd">            tensor([0.1000, 0.3000, 0.6000])</span>

<span class="sd">        Args:</span>
<span class="sd">            loss: Loss per item.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A tensor of probabilities for each item.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">square_loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">square_loss</span> <span class="o">/</span> <span class="n">square_loss</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">sample_input</span><span class="p">(</span>
        <span class="n">probabilities</span><span class="p">:</span> <span class="n">TrainBatchStatistic</span><span class="p">,</span>
        <span class="n">input_activations</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span><span class="p">,</span>
        <span class="n">num_samples</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">SampledDeadNeuronInputs</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Sample an input vector based on the provided probabilities.</span>

<span class="sd">        Example:</span>
<span class="sd">            &gt;&gt;&gt; probabilities = torch.tensor([0.1, 0.2, 0.7])</span>
<span class="sd">            &gt;&gt;&gt; input_activations = torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])</span>
<span class="sd">            &gt;&gt;&gt; _seed = torch.manual_seed(0)  # For reproducibility in example</span>
<span class="sd">            &gt;&gt;&gt; sampled_input = ActivationResampler.sample_input(</span>
<span class="sd">            ...     probabilities, input_activations, 2</span>
<span class="sd">            ... )</span>
<span class="sd">            &gt;&gt;&gt; sampled_input.tolist()</span>
<span class="sd">            [[5.0, 6.0], [3.0, 4.0]]</span>

<span class="sd">        Args:</span>
<span class="sd">            probabilities: Probabilities for each input.</span>
<span class="sd">            input_activations: Input activation vectors.</span>
<span class="sd">            num_samples: Number of samples to take (number of dead neurons).</span>

<span class="sd">        Returns:</span>
<span class="sd">            Sampled input activation vector.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If the number of samples is greater than the number of input activations.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">num_samples</span> <span class="o">&gt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_activations</span><span class="p">):</span>
            <span class="n">exception_message</span> <span class="o">=</span> <span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Cannot sample </span><span class="si">{</span><span class="n">num_samples</span><span class="si">}</span><span class="s2"> inputs from &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">input_activations</span><span class="p">)</span><span class="si">}</span><span class="s2"> input activations.&quot;</span>
            <span class="p">)</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">exception_message</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">num_samples</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
                <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">input_activations</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">input_activations</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                <span class="n">device</span><span class="o">=</span><span class="n">input_activations</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="n">sample_indices</span><span class="p">:</span> <span class="n">LearntNeuronIndices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span>
            <span class="n">probabilities</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="n">num_samples</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">input_activations</span><span class="p">[</span><span class="n">sample_indices</span><span class="p">,</span> <span class="p">:]</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">renormalize_and_scale</span><span class="p">(</span>
        <span class="n">sampled_input</span><span class="p">:</span> <span class="n">SampledDeadNeuronInputs</span><span class="p">,</span>
        <span class="n">neuron_activity</span><span class="p">:</span> <span class="n">NeuronActivity</span><span class="p">,</span>
        <span class="n">encoder_weight</span><span class="p">:</span> <span class="n">EncoderWeights</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">DeadEncoderNeuronWeightUpdates</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Renormalize and scale the resampled dictionary vectors.</span>

<span class="sd">        Renormalize the input vector to equal the average norm of the encoder weights for alive</span>
<span class="sd">        neurons times 0.2.</span>

<span class="sd">        Example:</span>
<span class="sd">            &gt;&gt;&gt; _seed = torch.manual_seed(0)  # For reproducibility in example</span>
<span class="sd">            &gt;&gt;&gt; sampled_input = torch.tensor([[3.0, 4.0]])</span>
<span class="sd">            &gt;&gt;&gt; neuron_activity = torch.tensor([3, 0, 5, 0, 1, 3])</span>
<span class="sd">            &gt;&gt;&gt; encoder_weight = torch.ones((6, 2))</span>
<span class="sd">            &gt;&gt;&gt; rescaled_input = ActivationResampler.renormalize_and_scale(</span>
<span class="sd">            ...     sampled_input,</span>
<span class="sd">            ...     neuron_activity,</span>
<span class="sd">            ...     encoder_weight</span>
<span class="sd">            ... )</span>
<span class="sd">            &gt;&gt;&gt; rescaled_input.round(decimals=1)</span>
<span class="sd">            tensor([[0.2000, 0.2000]])</span>

<span class="sd">        Args:</span>
<span class="sd">            sampled_input: Tensor of the sampled input activation.</span>
<span class="sd">            neuron_activity: Tensor representing the number of times each neuron fired.</span>
<span class="sd">            encoder_weight: Tensor of encoder weights.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Rescaled sampled input.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If there are no alive neurons.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">alive_neuron_mask</span><span class="p">:</span> <span class="n">Bool</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="s2">&quot; learned_features&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">neuron_activity</span> <span class="o">&gt;</span> <span class="mi">0</span>

        <span class="c1"># Check there is at least one alive neuron</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">alive_neuron_mask</span><span class="p">):</span>
            <span class="n">error_message</span> <span class="o">=</span> <span class="s2">&quot;No alive neurons found.&quot;</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">error_message</span><span class="p">)</span>

        <span class="c1"># Handle all alive neurons</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">alive_neuron_mask</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
                <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">sampled_input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sampled_input</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">sampled_input</span><span class="o">.</span><span class="n">device</span>
            <span class="p">)</span>

        <span class="c1"># Calculate the average norm of the encoder weights for alive neurons.</span>
        <span class="n">alive_encoder_weights</span><span class="p">:</span> <span class="n">AliveEncoderWeights</span> <span class="o">=</span> <span class="n">encoder_weight</span><span class="p">[</span><span class="n">alive_neuron_mask</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">average_alive_norm</span><span class="p">:</span> <span class="n">ItemTensor</span> <span class="o">=</span> <span class="n">alive_encoder_weights</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

        <span class="c1"># Renormalize the input vector to equal the average norm of the encoder weights for alive</span>
        <span class="c1"># neurons times 0.2.</span>
        <span class="n">renormalized_input</span><span class="p">:</span> <span class="n">SampledDeadNeuronInputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span>
            <span class="n">sampled_input</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">renormalized_input</span> <span class="o">*</span> <span class="p">(</span><span class="n">average_alive_norm</span> <span class="o">*</span> <span class="mf">0.2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">resample_dead_neurons</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">neuron_activity</span><span class="p">:</span> <span class="n">NeuronActivity</span><span class="p">,</span>
        <span class="n">activation_store</span><span class="p">:</span> <span class="n">ActivationStore</span><span class="p">,</span>
        <span class="n">autoencoder</span><span class="p">:</span> <span class="n">SparseAutoencoder</span><span class="p">,</span>
        <span class="n">loss_fn</span><span class="p">:</span> <span class="n">AbstractLoss</span><span class="p">,</span>
        <span class="n">train_batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_inputs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">819_200</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ParameterUpdateResults</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Resample dead neurons.</span>

<span class="sd">        Args:</span>
<span class="sd">            neuron_activity: Number of times each neuron fired.</span>
<span class="sd">            activation_store: Activation store.</span>
<span class="sd">            autoencoder: Sparse autoencoder model.</span>
<span class="sd">            loss_fn: Loss function.</span>
<span class="sd">            train_batch_size: Train batch size (also used for resampling).</span>
<span class="sd">            num_inputs: Number of input activations to use when resampling. Will be rounded down</span>
<span class="sd">                to divisible by the batch size, and cannot be larger than the number of items</span>
<span class="sd">                currently in the store.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">dead_neuron_indices</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_dead_neuron_indices</span><span class="p">(</span><span class="n">neuron_activity</span><span class="p">)</span>

            <span class="c1"># Compute the loss for the current model on a random subset of inputs and get the</span>
            <span class="c1"># activations.</span>
            <span class="n">loss</span><span class="p">,</span> <span class="n">input_activations</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_loss_and_get_activations</span><span class="p">(</span>
                <span class="n">store</span><span class="o">=</span><span class="n">activation_store</span><span class="p">,</span>
                <span class="n">autoencoder</span><span class="o">=</span><span class="n">autoencoder</span><span class="p">,</span>
                <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
                <span class="n">num_inputs</span><span class="o">=</span><span class="n">num_inputs</span><span class="p">,</span>
                <span class="n">train_batch_size</span><span class="o">=</span><span class="n">train_batch_size</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="c1"># Assign each input vector a probability of being picked that is proportional to the</span>
            <span class="c1"># square of the autoencoder&#39;s loss on that input.</span>
            <span class="n">sample_probabilities</span><span class="p">:</span> <span class="n">TrainBatchStatistic</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">assign_sampling_probabilities</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

            <span class="c1"># Get references to the encoder and decoder parameters</span>
            <span class="n">encoder_weight</span><span class="p">:</span> <span class="n">EncoderWeights</span> <span class="o">=</span> <span class="n">autoencoder</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">weight</span>

            <span class="c1"># For each dead neuron sample an input according to these probabilities.</span>
            <span class="n">sampled_input</span><span class="p">:</span> <span class="n">SampledDeadNeuronInputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample_input</span><span class="p">(</span>
                <span class="n">sample_probabilities</span><span class="p">,</span> <span class="n">input_activations</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">dead_neuron_indices</span><span class="p">)</span>
            <span class="p">)</span>

            <span class="c1"># Renormalize the input vector to have unit L2 norm and set this to be the dictionary</span>
            <span class="c1"># vector for the dead autoencoder neuron.</span>
            <span class="n">renormalized_input</span><span class="p">:</span> <span class="n">SampledDeadNeuronInputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span>
                <span class="n">sampled_input</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
            <span class="p">)</span>

            <span class="n">dead_decoder_weight_updates</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span>
                <span class="n">renormalized_input</span><span class="p">,</span> <span class="s2">&quot;dead_neuron input_feature -&gt; input_feature dead_neuron&quot;</span>
            <span class="p">)</span>

            <span class="c1"># For the corresponding encoder vector, renormalize the input vector to equal the</span>
            <span class="c1"># average norm of the encoder weights for alive neurons times 0.2. Set the corresponding</span>
            <span class="c1"># encoder bias element to zero.</span>
            <span class="n">rescaled_sampled_input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">renormalize_and_scale</span><span class="p">(</span>
                <span class="n">sampled_input</span><span class="p">,</span> <span class="n">neuron_activity</span><span class="p">,</span> <span class="n">encoder_weight</span>
            <span class="p">)</span>
            <span class="n">dead_encoder_bias_updates</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">dead_neuron_indices</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>

            <span class="k">return</span> <span class="n">ParameterUpdateResults</span><span class="p">(</span>
                <span class="n">dead_neuron_indices</span><span class="o">=</span><span class="n">dead_neuron_indices</span><span class="p">,</span>
                <span class="n">dead_encoder_weight_updates</span><span class="o">=</span><span class="n">rescaled_sampled_input</span><span class="p">,</span>
                <span class="n">dead_encoder_bias_updates</span><span class="o">=</span><span class="n">dead_encoder_bias_updates</span><span class="p">,</span>
                <span class="n">dead_decoder_weight_updates</span><span class="o">=</span><span class="n">dead_decoder_weight_updates</span><span class="p">,</span>
            <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.activation_resampler.ActivationResampler.assign_sampling_probabilities" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">assign_sampling_probabilities</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>Assign the sampling probabilities for each input activations vector.</p>
<p>Assign each input vector a probability of being picked that is proportional to the square of
the autoencoder's loss on that input.</p>

<details class="example" open>
  <summary>Example</summary>
  <blockquote>
<blockquote>
<blockquote>
<p>loss = torch.tensor([1.0, 2.0, 3.0])
ActivationResampler.assign_sampling_probabilities(loss).round(decimals=1)
tensor([0.1000, 0.3000, 0.6000])</p>
</blockquote>
</blockquote>
</blockquote>
</details>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>loss</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.TrainBatchStatistic" href="../tensor_types/#sparse_autoencoder.tensor_types.TrainBatchStatistic">TrainBatchStatistic</a></code>)
              –
              <div class="doc-md-description">
                <p>Loss per item.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
    <th class="field-name">Returns:</th>
    <td class="field-body">
      <ul class="first simple">
          <li>
                <code><span title="torch.Tensor">Tensor</span></code>
            –
            <div class="doc-md-description">
              <p>A tensor of probabilities for each item.</p>
            </div>
          </li>
      </ul>
    </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary> <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">assign_sampling_probabilities</span><span class="p">(</span><span class="n">loss</span><span class="p">:</span> <span class="n">TrainBatchStatistic</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Assign the sampling probabilities for each input activations vector.</span>

<span class="sd">    Assign each input vector a probability of being picked that is proportional to the square of</span>
<span class="sd">    the autoencoder&#39;s loss on that input.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; loss = torch.tensor([1.0, 2.0, 3.0])</span>
<span class="sd">        &gt;&gt;&gt; ActivationResampler.assign_sampling_probabilities(loss).round(decimals=1)</span>
<span class="sd">        tensor([0.1000, 0.3000, 0.6000])</span>

<span class="sd">    Args:</span>
<span class="sd">        loss: Loss per item.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A tensor of probabilities for each item.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">square_loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">square_loss</span> <span class="o">/</span> <span class="n">square_loss</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.activation_resampler.ActivationResampler.compute_loss_and_get_activations" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">compute_loss_and_get_activations</span><span class="p">(</span><span class="n">store</span><span class="p">,</span> <span class="n">autoencoder</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">num_inputs</span><span class="p">,</span> <span class="n">train_batch_size</span><span class="p">)</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>Compute the loss on a random subset of inputs.</p>
<p>Computes the loss and also stores the input activations (for use in resampling neurons).</p>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>store</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.activation_store.base_store.ActivationStore" href="../activation_store/base_store/#sparse_autoencoder.activation_store.base_store.ActivationStore">ActivationStore</a></code>)
              –
              <div class="doc-md-description">
                <p>Activation store.</p>
              </div>
            </li>
            <li>
              <b><code>autoencoder</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.autoencoder.model.SparseAutoencoder" href="../autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder">SparseAutoencoder</a></code>)
              –
              <div class="doc-md-description">
                <p>Sparse autoencoder model.</p>
              </div>
            </li>
            <li>
              <b><code>loss_fn</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.loss.abstract_loss.AbstractLoss" href="../loss/abstract_loss/#sparse_autoencoder.loss.abstract_loss.AbstractLoss">AbstractLoss</a></code>)
              –
              <div class="doc-md-description">
                <p>Loss function.</p>
              </div>
            </li>
            <li>
              <b><code>num_inputs</code></b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>Number of input activations to use.</p>
              </div>
            </li>
            <li>
              <b><code>train_batch_size</code></b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>Train batch size (also used for resampling).</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
    <th class="field-name">Returns:</th>
    <td class="field-body">
      <ul class="first simple">
          <li>
                <code>tuple[<a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.TrainBatchStatistic" href="../tensor_types/#sparse_autoencoder.tensor_types.TrainBatchStatistic">TrainBatchStatistic</a>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.InputOutputActivationBatch" href="../tensor_types/#sparse_autoencoder.tensor_types.InputOutputActivationBatch">InputOutputActivationBatch</a>]</code>
            –
            <div class="doc-md-description">
              <p>A tuple containing the loss per item, and all input activations.</p>
            </div>
          </li>
      </ul>
    </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary> <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">compute_loss_and_get_activations</span><span class="p">(</span>
    <span class="n">store</span><span class="p">:</span> <span class="n">ActivationStore</span><span class="p">,</span>
    <span class="n">autoencoder</span><span class="p">:</span> <span class="n">SparseAutoencoder</span><span class="p">,</span>
    <span class="n">loss_fn</span><span class="p">:</span> <span class="n">AbstractLoss</span><span class="p">,</span>
    <span class="n">num_inputs</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">train_batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">TrainBatchStatistic</span><span class="p">,</span> <span class="n">InputOutputActivationBatch</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute the loss on a random subset of inputs.</span>

<span class="sd">    Computes the loss and also stores the input activations (for use in resampling neurons).</span>

<span class="sd">    Args:</span>
<span class="sd">        store: Activation store.</span>
<span class="sd">        autoencoder: Sparse autoencoder model.</span>
<span class="sd">        loss_fn: Loss function.</span>
<span class="sd">        num_inputs: Number of input activations to use.</span>
<span class="sd">        train_batch_size: Train batch size (also used for resampling).</span>

<span class="sd">    Returns:</span>
<span class="sd">        A tuple containing the loss per item, and all input activations.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">loss_batches</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">TrainBatchStatistic</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">input_activations_batches</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">InputOutputActivationBatch</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">store</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">train_batch_size</span><span class="p">)</span>
        <span class="n">batches</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">num_inputs</span> <span class="o">//</span> <span class="n">train_batch_size</span>

        <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">dataloader</span><span class="p">)):</span>
            <span class="n">input_activations_batches</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
            <span class="n">learned_activations</span><span class="p">,</span> <span class="n">reconstructed_activations</span> <span class="o">=</span> <span class="n">autoencoder</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
            <span class="n">loss_batches</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">loss_fn</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">learned_activations</span><span class="p">,</span> <span class="n">reconstructed_activations</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">batch_idx</span> <span class="o">&gt;=</span> <span class="n">batches</span><span class="p">:</span>
                <span class="k">break</span>

        <span class="n">loss_result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">loss_batches</span><span class="p">)</span>
        <span class="n">input_activations</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">input_activations_batches</span><span class="p">)</span>

        <span class="c1"># Check we generated enough data</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">loss_result</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">num_inputs</span><span class="p">:</span>
            <span class="n">error_message</span> <span class="o">=</span> <span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Cannot get </span><span class="si">{</span><span class="n">num_inputs</span><span class="si">}</span><span class="s2"> items from the store, &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;as only </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">loss_result</span><span class="p">)</span><span class="si">}</span><span class="s2"> were available.&quot;</span>
            <span class="p">)</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">error_message</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">loss_result</span><span class="p">,</span> <span class="n">input_activations</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.activation_resampler.ActivationResampler.get_dead_neuron_indices" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">get_dead_neuron_indices</span><span class="p">(</span><span class="n">neuron_activity</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>Identify the indices of neurons that have zero activity.</p>

<details class="example" open>
  <summary>Example</summary>
  <blockquote>
<blockquote>
<blockquote>
<p>neuron_activity = torch.tensor([0, 0, 3, 10, 0])
dead_neuron_indices = ActivationResampler.get_dead_neuron_indices(neuron_activity)
dead_neuron_indices.tolist()
[0, 1, 4]</p>
</blockquote>
</blockquote>
</blockquote>
</details>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>neuron_activity</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.NeuronActivity" href="../tensor_types/#sparse_autoencoder.tensor_types.NeuronActivity">NeuronActivity</a></code>)
              –
              <div class="doc-md-description">
                <p>Tensor representing the number of times each neuron fired.</p>
              </div>
            </li>
            <li>
              <b><code>threshold</code></b>
                  (<code>int</code>, default:
                      <code>0</code>
)
              –
              <div class="doc-md-description">
                <p>Threshold for determining if a neuron is dead (has fired less than this
number of times.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
    <th class="field-name">Returns:</th>
    <td class="field-body">
      <ul class="first simple">
          <li>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.LearntNeuronIndices" href="../tensor_types/#sparse_autoencoder.tensor_types.LearntNeuronIndices">LearntNeuronIndices</a></code>
            –
            <div class="doc-md-description">
              <p>A tensor containing the indices of neurons that are 'dead' (zero activity).</p>
            </div>
          </li>
      </ul>
    </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary> <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">get_dead_neuron_indices</span><span class="p">(</span>
    <span class="n">neuron_activity</span><span class="p">:</span> <span class="n">NeuronActivity</span><span class="p">,</span> <span class="n">threshold</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">LearntNeuronIndices</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Identify the indices of neurons that have zero activity.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; neuron_activity = torch.tensor([0, 0, 3, 10, 0])</span>
<span class="sd">        &gt;&gt;&gt; dead_neuron_indices = ActivationResampler.get_dead_neuron_indices(neuron_activity)</span>
<span class="sd">        &gt;&gt;&gt; dead_neuron_indices.tolist()</span>
<span class="sd">        [0, 1, 4]</span>

<span class="sd">    Args:</span>
<span class="sd">        neuron_activity: Tensor representing the number of times each neuron fired.</span>
<span class="sd">        threshold: Threshold for determining if a neuron is dead (has fired less than this</span>
<span class="sd">            number of times.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A tensor containing the indices of neurons that are &#39;dead&#39; (zero activity).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">neuron_activity</span> <span class="o">&lt;=</span> <span class="n">threshold</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.activation_resampler.ActivationResampler.renormalize_and_scale" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">renormalize_and_scale</span><span class="p">(</span><span class="n">sampled_input</span><span class="p">,</span> <span class="n">neuron_activity</span><span class="p">,</span> <span class="n">encoder_weight</span><span class="p">)</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>Renormalize and scale the resampled dictionary vectors.</p>
<p>Renormalize the input vector to equal the average norm of the encoder weights for alive
neurons times 0.2.</p>

<details class="example" open>
  <summary>Example</summary>
  <blockquote>
<blockquote>
<blockquote>
<p>_seed = torch.manual_seed(0)  # For reproducibility in example
sampled_input = torch.tensor([[3.0, 4.0]])
neuron_activity = torch.tensor([3, 0, 5, 0, 1, 3])
encoder_weight = torch.ones((6, 2))
rescaled_input = ActivationResampler.renormalize_and_scale(
...     sampled_input,
...     neuron_activity,
...     encoder_weight
... )
rescaled_input.round(decimals=1)
tensor([[0.2000, 0.2000]])</p>
</blockquote>
</blockquote>
</blockquote>
</details>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>sampled_input</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.SampledDeadNeuronInputs" href="../tensor_types/#sparse_autoencoder.tensor_types.SampledDeadNeuronInputs">SampledDeadNeuronInputs</a></code>)
              –
              <div class="doc-md-description">
                <p>Tensor of the sampled input activation.</p>
              </div>
            </li>
            <li>
              <b><code>neuron_activity</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.NeuronActivity" href="../tensor_types/#sparse_autoencoder.tensor_types.NeuronActivity">NeuronActivity</a></code>)
              –
              <div class="doc-md-description">
                <p>Tensor representing the number of times each neuron fired.</p>
              </div>
            </li>
            <li>
              <b><code>encoder_weight</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.EncoderWeights" href="../tensor_types/#sparse_autoencoder.tensor_types.EncoderWeights">EncoderWeights</a></code>)
              –
              <div class="doc-md-description">
                <p>Tensor of encoder weights.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
    <th class="field-name">Returns:</th>
    <td class="field-body">
      <ul class="first simple">
          <li>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.DeadEncoderNeuronWeightUpdates" href="../tensor_types/#sparse_autoencoder.tensor_types.DeadEncoderNeuronWeightUpdates">DeadEncoderNeuronWeightUpdates</a></code>
            –
            <div class="doc-md-description">
              <p>Rescaled sampled input.</p>
            </div>
          </li>
      </ul>
    </td>
    </tr>
  </tbody>
</table>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Raises:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
                  <code>ValueError</code>
              –
              <div class="doc-md-description">
                <p>If there are no alive neurons.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary> <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">renormalize_and_scale</span><span class="p">(</span>
    <span class="n">sampled_input</span><span class="p">:</span> <span class="n">SampledDeadNeuronInputs</span><span class="p">,</span>
    <span class="n">neuron_activity</span><span class="p">:</span> <span class="n">NeuronActivity</span><span class="p">,</span>
    <span class="n">encoder_weight</span><span class="p">:</span> <span class="n">EncoderWeights</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">DeadEncoderNeuronWeightUpdates</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Renormalize and scale the resampled dictionary vectors.</span>

<span class="sd">    Renormalize the input vector to equal the average norm of the encoder weights for alive</span>
<span class="sd">    neurons times 0.2.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; _seed = torch.manual_seed(0)  # For reproducibility in example</span>
<span class="sd">        &gt;&gt;&gt; sampled_input = torch.tensor([[3.0, 4.0]])</span>
<span class="sd">        &gt;&gt;&gt; neuron_activity = torch.tensor([3, 0, 5, 0, 1, 3])</span>
<span class="sd">        &gt;&gt;&gt; encoder_weight = torch.ones((6, 2))</span>
<span class="sd">        &gt;&gt;&gt; rescaled_input = ActivationResampler.renormalize_and_scale(</span>
<span class="sd">        ...     sampled_input,</span>
<span class="sd">        ...     neuron_activity,</span>
<span class="sd">        ...     encoder_weight</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt; rescaled_input.round(decimals=1)</span>
<span class="sd">        tensor([[0.2000, 0.2000]])</span>

<span class="sd">    Args:</span>
<span class="sd">        sampled_input: Tensor of the sampled input activation.</span>
<span class="sd">        neuron_activity: Tensor representing the number of times each neuron fired.</span>
<span class="sd">        encoder_weight: Tensor of encoder weights.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Rescaled sampled input.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If there are no alive neurons.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">alive_neuron_mask</span><span class="p">:</span> <span class="n">Bool</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="s2">&quot; learned_features&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">neuron_activity</span> <span class="o">&gt;</span> <span class="mi">0</span>

    <span class="c1"># Check there is at least one alive neuron</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">alive_neuron_mask</span><span class="p">):</span>
        <span class="n">error_message</span> <span class="o">=</span> <span class="s2">&quot;No alive neurons found.&quot;</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">error_message</span><span class="p">)</span>

    <span class="c1"># Handle all alive neurons</span>
    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">alive_neuron_mask</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
            <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">sampled_input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sampled_input</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">sampled_input</span><span class="o">.</span><span class="n">device</span>
        <span class="p">)</span>

    <span class="c1"># Calculate the average norm of the encoder weights for alive neurons.</span>
    <span class="n">alive_encoder_weights</span><span class="p">:</span> <span class="n">AliveEncoderWeights</span> <span class="o">=</span> <span class="n">encoder_weight</span><span class="p">[</span><span class="n">alive_neuron_mask</span><span class="p">,</span> <span class="p">:]</span>
    <span class="n">average_alive_norm</span><span class="p">:</span> <span class="n">ItemTensor</span> <span class="o">=</span> <span class="n">alive_encoder_weights</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

    <span class="c1"># Renormalize the input vector to equal the average norm of the encoder weights for alive</span>
    <span class="c1"># neurons times 0.2.</span>
    <span class="n">renormalized_input</span><span class="p">:</span> <span class="n">SampledDeadNeuronInputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span>
        <span class="n">sampled_input</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">renormalized_input</span> <span class="o">*</span> <span class="p">(</span><span class="n">average_alive_norm</span> <span class="o">*</span> <span class="mf">0.2</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.activation_resampler.ActivationResampler.resample_dead_neurons" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">resample_dead_neurons</span><span class="p">(</span><span class="n">neuron_activity</span><span class="p">,</span> <span class="n">activation_store</span><span class="p">,</span> <span class="n">autoencoder</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">train_batch_size</span><span class="p">,</span> <span class="n">num_inputs</span><span class="o">=</span><span class="mi">819200</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Resample dead neurons.</p>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>neuron_activity</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.NeuronActivity" href="../tensor_types/#sparse_autoencoder.tensor_types.NeuronActivity">NeuronActivity</a></code>)
              –
              <div class="doc-md-description">
                <p>Number of times each neuron fired.</p>
              </div>
            </li>
            <li>
              <b><code>activation_store</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.activation_store.base_store.ActivationStore" href="../activation_store/base_store/#sparse_autoencoder.activation_store.base_store.ActivationStore">ActivationStore</a></code>)
              –
              <div class="doc-md-description">
                <p>Activation store.</p>
              </div>
            </li>
            <li>
              <b><code>autoencoder</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.autoencoder.model.SparseAutoencoder" href="../autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder">SparseAutoencoder</a></code>)
              –
              <div class="doc-md-description">
                <p>Sparse autoencoder model.</p>
              </div>
            </li>
            <li>
              <b><code>loss_fn</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.loss.abstract_loss.AbstractLoss" href="../loss/abstract_loss/#sparse_autoencoder.loss.abstract_loss.AbstractLoss">AbstractLoss</a></code>)
              –
              <div class="doc-md-description">
                <p>Loss function.</p>
              </div>
            </li>
            <li>
              <b><code>train_batch_size</code></b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>Train batch size (also used for resampling).</p>
              </div>
            </li>
            <li>
              <b><code>num_inputs</code></b>
                  (<code>int</code>, default:
                      <code>819200</code>
)
              –
              <div class="doc-md-description">
                <p>Number of input activations to use when resampling. Will be rounded down
to divisible by the batch size, and cannot be larger than the number of items
currently in the store.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary> <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">resample_dead_neurons</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">neuron_activity</span><span class="p">:</span> <span class="n">NeuronActivity</span><span class="p">,</span>
    <span class="n">activation_store</span><span class="p">:</span> <span class="n">ActivationStore</span><span class="p">,</span>
    <span class="n">autoencoder</span><span class="p">:</span> <span class="n">SparseAutoencoder</span><span class="p">,</span>
    <span class="n">loss_fn</span><span class="p">:</span> <span class="n">AbstractLoss</span><span class="p">,</span>
    <span class="n">train_batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">num_inputs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">819_200</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ParameterUpdateResults</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Resample dead neurons.</span>

<span class="sd">    Args:</span>
<span class="sd">        neuron_activity: Number of times each neuron fired.</span>
<span class="sd">        activation_store: Activation store.</span>
<span class="sd">        autoencoder: Sparse autoencoder model.</span>
<span class="sd">        loss_fn: Loss function.</span>
<span class="sd">        train_batch_size: Train batch size (also used for resampling).</span>
<span class="sd">        num_inputs: Number of input activations to use when resampling. Will be rounded down</span>
<span class="sd">            to divisible by the batch size, and cannot be larger than the number of items</span>
<span class="sd">            currently in the store.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">dead_neuron_indices</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_dead_neuron_indices</span><span class="p">(</span><span class="n">neuron_activity</span><span class="p">)</span>

        <span class="c1"># Compute the loss for the current model on a random subset of inputs and get the</span>
        <span class="c1"># activations.</span>
        <span class="n">loss</span><span class="p">,</span> <span class="n">input_activations</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_loss_and_get_activations</span><span class="p">(</span>
            <span class="n">store</span><span class="o">=</span><span class="n">activation_store</span><span class="p">,</span>
            <span class="n">autoencoder</span><span class="o">=</span><span class="n">autoencoder</span><span class="p">,</span>
            <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
            <span class="n">num_inputs</span><span class="o">=</span><span class="n">num_inputs</span><span class="p">,</span>
            <span class="n">train_batch_size</span><span class="o">=</span><span class="n">train_batch_size</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Assign each input vector a probability of being picked that is proportional to the</span>
        <span class="c1"># square of the autoencoder&#39;s loss on that input.</span>
        <span class="n">sample_probabilities</span><span class="p">:</span> <span class="n">TrainBatchStatistic</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">assign_sampling_probabilities</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

        <span class="c1"># Get references to the encoder and decoder parameters</span>
        <span class="n">encoder_weight</span><span class="p">:</span> <span class="n">EncoderWeights</span> <span class="o">=</span> <span class="n">autoencoder</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">weight</span>

        <span class="c1"># For each dead neuron sample an input according to these probabilities.</span>
        <span class="n">sampled_input</span><span class="p">:</span> <span class="n">SampledDeadNeuronInputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample_input</span><span class="p">(</span>
            <span class="n">sample_probabilities</span><span class="p">,</span> <span class="n">input_activations</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">dead_neuron_indices</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="c1"># Renormalize the input vector to have unit L2 norm and set this to be the dictionary</span>
        <span class="c1"># vector for the dead autoencoder neuron.</span>
        <span class="n">renormalized_input</span><span class="p">:</span> <span class="n">SampledDeadNeuronInputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span>
            <span class="n">sampled_input</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
        <span class="p">)</span>

        <span class="n">dead_decoder_weight_updates</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span>
            <span class="n">renormalized_input</span><span class="p">,</span> <span class="s2">&quot;dead_neuron input_feature -&gt; input_feature dead_neuron&quot;</span>
        <span class="p">)</span>

        <span class="c1"># For the corresponding encoder vector, renormalize the input vector to equal the</span>
        <span class="c1"># average norm of the encoder weights for alive neurons times 0.2. Set the corresponding</span>
        <span class="c1"># encoder bias element to zero.</span>
        <span class="n">rescaled_sampled_input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">renormalize_and_scale</span><span class="p">(</span>
            <span class="n">sampled_input</span><span class="p">,</span> <span class="n">neuron_activity</span><span class="p">,</span> <span class="n">encoder_weight</span>
        <span class="p">)</span>
        <span class="n">dead_encoder_bias_updates</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">dead_neuron_indices</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">ParameterUpdateResults</span><span class="p">(</span>
            <span class="n">dead_neuron_indices</span><span class="o">=</span><span class="n">dead_neuron_indices</span><span class="p">,</span>
            <span class="n">dead_encoder_weight_updates</span><span class="o">=</span><span class="n">rescaled_sampled_input</span><span class="p">,</span>
            <span class="n">dead_encoder_bias_updates</span><span class="o">=</span><span class="n">dead_encoder_bias_updates</span><span class="p">,</span>
            <span class="n">dead_decoder_weight_updates</span><span class="o">=</span><span class="n">dead_decoder_weight_updates</span><span class="p">,</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.activation_resampler.ActivationResampler.sample_input" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">sample_input</span><span class="p">(</span><span class="n">probabilities</span><span class="p">,</span> <span class="n">input_activations</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">)</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>Sample an input vector based on the provided probabilities.</p>

<details class="example" open>
  <summary>Example</summary>
  <blockquote>
<blockquote>
<blockquote>
<p>probabilities = torch.tensor([0.1, 0.2, 0.7])
input_activations = torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])
_seed = torch.manual_seed(0)  # For reproducibility in example
sampled_input = ActivationResampler.sample_input(
...     probabilities, input_activations, 2
... )
sampled_input.tolist()
[[5.0, 6.0], [3.0, 4.0]]</p>
</blockquote>
</blockquote>
</blockquote>
</details>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>probabilities</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.TrainBatchStatistic" href="../tensor_types/#sparse_autoencoder.tensor_types.TrainBatchStatistic">TrainBatchStatistic</a></code>)
              –
              <div class="doc-md-description">
                <p>Probabilities for each input.</p>
              </div>
            </li>
            <li>
              <b><code>input_activations</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.InputOutputActivationBatch" href="../tensor_types/#sparse_autoencoder.tensor_types.InputOutputActivationBatch">InputOutputActivationBatch</a></code>)
              –
              <div class="doc-md-description">
                <p>Input activation vectors.</p>
              </div>
            </li>
            <li>
              <b><code>num_samples</code></b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>Number of samples to take (number of dead neurons).</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
    <th class="field-name">Returns:</th>
    <td class="field-body">
      <ul class="first simple">
          <li>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.SampledDeadNeuronInputs" href="../tensor_types/#sparse_autoencoder.tensor_types.SampledDeadNeuronInputs">SampledDeadNeuronInputs</a></code>
            –
            <div class="doc-md-description">
              <p>Sampled input activation vector.</p>
            </div>
          </li>
      </ul>
    </td>
    </tr>
  </tbody>
</table>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Raises:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
                  <code>ValueError</code>
              –
              <div class="doc-md-description">
                <p>If the number of samples is greater than the number of input activations.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary> <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">sample_input</span><span class="p">(</span>
    <span class="n">probabilities</span><span class="p">:</span> <span class="n">TrainBatchStatistic</span><span class="p">,</span>
    <span class="n">input_activations</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span><span class="p">,</span>
    <span class="n">num_samples</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">SampledDeadNeuronInputs</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Sample an input vector based on the provided probabilities.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; probabilities = torch.tensor([0.1, 0.2, 0.7])</span>
<span class="sd">        &gt;&gt;&gt; input_activations = torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])</span>
<span class="sd">        &gt;&gt;&gt; _seed = torch.manual_seed(0)  # For reproducibility in example</span>
<span class="sd">        &gt;&gt;&gt; sampled_input = ActivationResampler.sample_input(</span>
<span class="sd">        ...     probabilities, input_activations, 2</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt; sampled_input.tolist()</span>
<span class="sd">        [[5.0, 6.0], [3.0, 4.0]]</span>

<span class="sd">    Args:</span>
<span class="sd">        probabilities: Probabilities for each input.</span>
<span class="sd">        input_activations: Input activation vectors.</span>
<span class="sd">        num_samples: Number of samples to take (number of dead neurons).</span>

<span class="sd">    Returns:</span>
<span class="sd">        Sampled input activation vector.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the number of samples is greater than the number of input activations.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">num_samples</span> <span class="o">&gt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_activations</span><span class="p">):</span>
        <span class="n">exception_message</span> <span class="o">=</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Cannot sample </span><span class="si">{</span><span class="n">num_samples</span><span class="si">}</span><span class="s2"> inputs from &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">input_activations</span><span class="p">)</span><span class="si">}</span><span class="s2"> input activations.&quot;</span>
        <span class="p">)</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">exception_message</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">num_samples</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
            <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">input_activations</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">input_activations</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">input_activations</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="n">sample_indices</span><span class="p">:</span> <span class="n">LearntNeuronIndices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span>
        <span class="n">probabilities</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="n">num_samples</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">input_activations</span><span class="p">[</span><span class="n">sample_indices</span><span class="p">,</span> <span class="p">:]</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">




<h2 id="sparse_autoencoder.activation_resampler.ParameterUpdateResults" class="doc doc-heading">
          <code>ParameterUpdateResults</code>

  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

</h2>


  <div class="doc doc-contents ">

  
      <p>Parameter update results from resampling dead neurons.</p>

            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/activation_resampler/abstract_activation_resampler.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">ParameterUpdateResults</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Parameter update results from resampling dead neurons.&quot;&quot;&quot;</span>

    <span class="n">dead_neuron_indices</span><span class="p">:</span> <span class="n">LearntNeuronIndices</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Dead neuron indices.&quot;&quot;&quot;</span>

    <span class="n">dead_encoder_weight_updates</span><span class="p">:</span> <span class="n">DeadEncoderNeuronWeightUpdates</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Dead encoder weight updates.&quot;&quot;&quot;</span>

    <span class="n">dead_encoder_bias_updates</span><span class="p">:</span> <span class="n">DeadEncoderNeuronBiasUpdates</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Dead encoder bias updates.&quot;&quot;&quot;</span>

    <span class="n">dead_decoder_weight_updates</span><span class="p">:</span> <span class="n">DeadDecoderNeuronWeightUpdates</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Dead decoder weight updates.&quot;&quot;&quot;</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">




<h3 id="sparse_autoencoder.activation_resampler.ParameterUpdateResults.dead_decoder_weight_updates" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">dead_decoder_weight_updates</span><span class="p">:</span> <span class="n">DeadDecoderNeuronWeightUpdates</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>Dead decoder weight updates.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">




<h3 id="sparse_autoencoder.activation_resampler.ParameterUpdateResults.dead_encoder_bias_updates" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">dead_encoder_bias_updates</span><span class="p">:</span> <span class="n">DeadEncoderNeuronBiasUpdates</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>Dead encoder bias updates.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">




<h3 id="sparse_autoencoder.activation_resampler.ParameterUpdateResults.dead_encoder_weight_updates" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">dead_encoder_weight_updates</span><span class="p">:</span> <span class="n">DeadEncoderNeuronWeightUpdates</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>Dead encoder weight updates.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">




<h3 id="sparse_autoencoder.activation_resampler.ParameterUpdateResults.dead_neuron_indices" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">dead_neuron_indices</span><span class="p">:</span> <span class="n">LearntNeuronIndices</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>Dead neuron indices.</p>
  </div>

</div>





  </div>

  </div>

</div>




  </div>

  </div>

</div>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../" class="btn btn-neutral float-left" title="Home"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="abstract_activation_resampler/" class="btn btn-neutral float-right" title="abstract_activation_resampler">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="abstract_activation_resampler/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "../..";</script>
    <script src="../../js/theme_extra.js"></script>
    <script src="../../js/theme.js"></script>
      <script src="../../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
