<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../../img/favicon.ico" />
    <title>Index - Sparse Autoencoder</title>
    <link rel="stylesheet" href="../../css/theme.css" />
    <link rel="stylesheet" href="../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="../../assets/_mkdocstrings.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Index";
        var mkdocs_page_input_path = "reference/autoencoder/index.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../.." class="icon icon-home"> Sparse Autoencoder
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../..">Home</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../../getting_started/">Getting Started</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../../contributing/">Contributing</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../../citation/">Citation</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Reference</span></p>
              <ul class="current">
                  <li class="toctree-l1"><a class="reference internal" href="../">Home</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">activation_resampler</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../activation_resampler/">Index</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../activation_resampler/abstract_activation_resampler/">abstract_activation_resampler</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../activation_resampler/activation_resampler/">activation_resampler</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">activation_store</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../activation_store/">Index</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../activation_store/base_store/">base_store</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../activation_store/disk_store/">disk_store</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../activation_store/list_store/">list_store</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../activation_store/tensor_store/">tensor_store</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="#">utils</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../activation_store/utils/">Index</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../activation_store/utils/extend_resize/">extend_resize</a>
                </li>
    </ul>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1 current"><a class="reference internal current" href="#">autoencoder</a>
    <ul class="current">
                <li class="toctree-l2 current"><a class="reference internal current" href="./">Index</a>
    <ul class="current">
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="abstract_autoencoder/">abstract_autoencoder</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="#">components</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="components/">Index</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="components/abstract_decoder/">abstract_decoder</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="components/abstract_encoder/">abstract_encoder</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="components/abstract_outer_bias/">abstract_outer_bias</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="components/linear_encoder/">linear_encoder</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="components/tied_bias/">tied_bias</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="components/unit_norm_decoder/">unit_norm_decoder</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="components/unit_norm_linear/">unit_norm_linear</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="fista_autoencoder/">fista_autoencoder</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="loss/">loss</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="model/">model</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">loss</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../loss/">Index</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../loss/abstract_loss/">abstract_loss</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../loss/fista/">fista</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../loss/learned_activations_l1/">learned_activations_l1</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../loss/mse_reconstruction_loss/">mse_reconstruction_loss</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../loss/reducer/">reducer</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">metrics</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../metrics/">Index</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../metrics/abstract_metric/">abstract_metric</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="#">generate</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../metrics/generate/">Index</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../metrics/generate/abstract_generate_metric/">abstract_generate_metric</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="#">train</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../metrics/train/">Index</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../metrics/train/abstract_train_metric/">abstract_train_metric</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="#">validate</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../metrics/validate/">Index</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../metrics/validate/abstract_validate_metric/">abstract_validate_metric</a>
                </li>
    </ul>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">optimizer</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../optimizer/">Index</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../optimizer/abstract_optimizer/">abstract_optimizer</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../optimizer/adam_with_reset/">adam_with_reset</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">source_data</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../source_data/">Index</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../source_data/abstract_dataset/">abstract_dataset</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../source_data/c4_pre_tokenized/">c4_pre_tokenized</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../source_data/pile_uncopyrighted/">pile_uncopyrighted</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../source_data/pretokenized_dataset/">pretokenized_dataset</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../source_data/random_int/">random_int</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../source_data/text_dataset/">text_dataset</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">source_model</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../source_model/">Index</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../source_model/store_activations_hook/">store_activations_hook</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">src_model</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../src_model/">Index</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../src_model/store_activations_hook/">store_activations_hook</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tensor_types/">tensor_types</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">train</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../train/">Index</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../train/abstract_pipeline/">abstract_pipeline</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../train/generate_activations/">generate_activations</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="#">metrics</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../train/metrics/">Index</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../train/metrics/capacity/">capacity</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../train/metrics/feature_density/">feature_density</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../train/pipeline/">pipeline</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../train/resample_neurons/">resample_neurons</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../train/sweep_config/">sweep_config</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../train/train_autoencoder/">train_autoencoder</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="#">utils</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../train/utils/">Index</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../train/utils/get_model_device/">get_model_device</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../train/utils/wandb_sweep_types/">wandb_sweep_types</a>
                </li>
    </ul>
                </li>
    </ul>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../..">Sparse Autoencoder</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../.." class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">Reference</li>
          <li class="breadcrumb-item">autoencoder</li>
      <li class="breadcrumb-item active">Index</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <div class="doc doc-object doc-module">



<a id="sparse_autoencoder.autoencoder"></a>
  <div class="doc doc-contents first">
  
      <p>Sparse autoencoder model &amp; components.</p>

  

  <div class="doc doc-children">








<div class="doc doc-object doc-class">




<h2 id="sparse_autoencoder.autoencoder.AbstractAutoencoder" class="doc doc-heading">
          <code>AbstractAutoencoder</code>


</h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><span title="torch.nn.Module">Module</span></code>, <code><span title="abc.ABC">ABC</span></code></p>

  
      <p>Abstract Sparse Autoencoder Model.</p>

            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/autoencoder/abstract_autoencoder.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">AbstractAutoencoder</span><span class="p">(</span><span class="n">Module</span><span class="p">,</span> <span class="n">ABC</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Abstract Sparse Autoencoder Model.&quot;&quot;&quot;</span>

    <span class="nd">@property</span>
    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">encoder</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AbstractEncoder</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Encoder.&quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="nd">@property</span>
    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">decoder</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AbstractDecoder</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Decoder.&quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="nd">@property</span>
    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">pre_encoder_bias</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AbstractOuterBias</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Pre-encoder bias.&quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="nd">@property</span>
    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">post_decoder_bias</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AbstractOuterBias</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Post-decoder bias.&quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span>
        <span class="n">LearnedActivationBatch</span><span class="p">,</span>
        <span class="n">InputOutputActivationBatch</span><span class="p">,</span>
    <span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Forward Pass.</span>

<span class="sd">        Args:</span>
<span class="sd">            x: Input activations (e.g. activations from an MLP layer in a transformer model).</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tuple of learned activations and decoded activations.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Reset the parameters.&quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">




<h3 id="sparse_autoencoder.autoencoder.AbstractAutoencoder.decoder" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">decoder</span><span class="p">:</span> <span class="n">AbstractDecoder</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-abstractmethod"><code>abstractmethod</code></small>
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>Decoder.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">




<h3 id="sparse_autoencoder.autoencoder.AbstractAutoencoder.encoder" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">encoder</span><span class="p">:</span> <span class="n">AbstractEncoder</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-abstractmethod"><code>abstractmethod</code></small>
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>Encoder.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">




<h3 id="sparse_autoencoder.autoencoder.AbstractAutoencoder.post_decoder_bias" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">post_decoder_bias</span><span class="p">:</span> <span class="n">AbstractOuterBias</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-abstractmethod"><code>abstractmethod</code></small>
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>Post-decoder bias.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">




<h3 id="sparse_autoencoder.autoencoder.AbstractAutoencoder.pre_encoder_bias" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">pre_encoder_bias</span><span class="p">:</span> <span class="n">AbstractOuterBias</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-abstractmethod"><code>abstractmethod</code></small>
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>Pre-encoder bias.</p>
  </div>

</div>




<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.autoencoder.AbstractAutoencoder.forward" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-abstractmethod"><code>abstractmethod</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>Forward Pass.</p>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>x</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.InputOutputActivationBatch" href="../tensor_types/#sparse_autoencoder.tensor_types.InputOutputActivationBatch">InputOutputActivationBatch</a></code>)
              –
              <div class="doc-md-description">
                <p>Input activations (e.g. activations from an MLP layer in a transformer model).</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
    <th class="field-name">Returns:</th>
    <td class="field-body">
      <ul class="first simple">
          <li>
                <code>tuple[<a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.LearnedActivationBatch" href="../tensor_types/#sparse_autoencoder.tensor_types.LearnedActivationBatch">LearnedActivationBatch</a>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.InputOutputActivationBatch" href="../tensor_types/#sparse_autoencoder.tensor_types.InputOutputActivationBatch">InputOutputActivationBatch</a>]</code>
            –
            <div class="doc-md-description">
              <p>Tuple of learned activations and decoded activations.</p>
            </div>
          </li>
      </ul>
    </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary> <code>sparse_autoencoder/autoencoder/abstract_autoencoder.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@abstractmethod</span>
<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span>
    <span class="n">LearnedActivationBatch</span><span class="p">,</span>
    <span class="n">InputOutputActivationBatch</span><span class="p">,</span>
<span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Forward Pass.</span>

<span class="sd">    Args:</span>
<span class="sd">        x: Input activations (e.g. activations from an MLP layer in a transformer model).</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tuple of learned activations and decoded activations.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.autoencoder.AbstractAutoencoder.reset_parameters" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">reset_parameters</span><span class="p">()</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-abstractmethod"><code>abstractmethod</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>Reset the parameters.</p>

          <details class="quote">
            <summary> <code>sparse_autoencoder/autoencoder/abstract_autoencoder.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@abstractmethod</span>
<span class="k">def</span> <span class="nf">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Reset the parameters.&quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">




<h2 id="sparse_autoencoder.autoencoder.AbstractDecoder" class="doc doc-heading">
          <code>AbstractDecoder</code>


</h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><span title="torch.nn.Module">Module</span></code>, <code><span title="abc.ABC">ABC</span></code></p>

  
      <p>Abstract Decoder Module.</p>
<p>Typically includes just a :attr:<code>weight</code> parameter.</p>

            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/autoencoder/components/abstract_decoder.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">AbstractDecoder</span><span class="p">(</span><span class="n">Module</span><span class="p">,</span> <span class="n">ABC</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Abstract Decoder Module.</span>

<span class="sd">    Typically includes just a :attr:`weight` parameter.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@property</span>
    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">weight</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">DecoderWeights</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Weight.&quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">LearnedActivationBatch</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">InputOutputActivationBatch</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Forward Pass.</span>

<span class="sd">        Args:</span>
<span class="sd">            x: Learned activations.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Decoded activations.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Reset the parameters.&quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="nd">@final</span>
    <span class="k">def</span> <span class="nf">update_dictionary_vectors</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">dictionary_vector_indices</span><span class="p">:</span> <span class="n">InputOutputNeuronIndices</span><span class="p">,</span>
        <span class="n">updated_weights</span><span class="p">:</span> <span class="n">DeadDecoderNeuronWeightUpdates</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Update decoder dictionary vectors.</span>

<span class="sd">        Updates the dictionary vectors (rows in the weight matrix) with the given values. Typically</span>
<span class="sd">        this is used when resampling neurons (dictionary vectors) that have died.</span>

<span class="sd">        Args:</span>
<span class="sd">            dictionary_vector_indices: Indices of the dictionary vectors to update.</span>
<span class="sd">            updated_weights: Updated weights for just these dictionary vectors.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">dictionary_vector_indices</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">[</span><span class="n">dictionary_vector_indices</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">updated_weights</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">




<h3 id="sparse_autoencoder.autoencoder.AbstractDecoder.weight" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">weight</span><span class="p">:</span> <span class="n">DecoderWeights</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-abstractmethod"><code>abstractmethod</code></small>
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>Weight.</p>
  </div>

</div>




<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.autoencoder.AbstractDecoder.forward" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-abstractmethod"><code>abstractmethod</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>Forward Pass.</p>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>x</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.LearnedActivationBatch" href="../tensor_types/#sparse_autoencoder.tensor_types.LearnedActivationBatch">LearnedActivationBatch</a></code>)
              –
              <div class="doc-md-description">
                <p>Learned activations.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
    <th class="field-name">Returns:</th>
    <td class="field-body">
      <ul class="first simple">
          <li>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.InputOutputActivationBatch" href="../tensor_types/#sparse_autoencoder.tensor_types.InputOutputActivationBatch">InputOutputActivationBatch</a></code>
            –
            <div class="doc-md-description">
              <p>Decoded activations.</p>
            </div>
          </li>
      </ul>
    </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary> <code>sparse_autoencoder/autoencoder/components/abstract_decoder.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@abstractmethod</span>
<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">LearnedActivationBatch</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">InputOutputActivationBatch</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Forward Pass.</span>

<span class="sd">    Args:</span>
<span class="sd">        x: Learned activations.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Decoded activations.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.autoencoder.AbstractDecoder.reset_parameters" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">reset_parameters</span><span class="p">()</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-abstractmethod"><code>abstractmethod</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>Reset the parameters.</p>

          <details class="quote">
            <summary> <code>sparse_autoencoder/autoencoder/components/abstract_decoder.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@abstractmethod</span>
<span class="k">def</span> <span class="nf">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Reset the parameters.&quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.autoencoder.AbstractDecoder.update_dictionary_vectors" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">update_dictionary_vectors</span><span class="p">(</span><span class="n">dictionary_vector_indices</span><span class="p">,</span> <span class="n">updated_weights</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Update decoder dictionary vectors.</p>
<p>Updates the dictionary vectors (rows in the weight matrix) with the given values. Typically
this is used when resampling neurons (dictionary vectors) that have died.</p>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>dictionary_vector_indices</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.InputOutputNeuronIndices" href="../tensor_types/#sparse_autoencoder.tensor_types.InputOutputNeuronIndices">InputOutputNeuronIndices</a></code>)
              –
              <div class="doc-md-description">
                <p>Indices of the dictionary vectors to update.</p>
              </div>
            </li>
            <li>
              <b><code>updated_weights</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.DeadDecoderNeuronWeightUpdates" href="../tensor_types/#sparse_autoencoder.tensor_types.DeadDecoderNeuronWeightUpdates">DeadDecoderNeuronWeightUpdates</a></code>)
              –
              <div class="doc-md-description">
                <p>Updated weights for just these dictionary vectors.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary> <code>sparse_autoencoder/autoencoder/components/abstract_decoder.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@final</span>
<span class="k">def</span> <span class="nf">update_dictionary_vectors</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">dictionary_vector_indices</span><span class="p">:</span> <span class="n">InputOutputNeuronIndices</span><span class="p">,</span>
    <span class="n">updated_weights</span><span class="p">:</span> <span class="n">DeadDecoderNeuronWeightUpdates</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Update decoder dictionary vectors.</span>

<span class="sd">    Updates the dictionary vectors (rows in the weight matrix) with the given values. Typically</span>
<span class="sd">    this is used when resampling neurons (dictionary vectors) that have died.</span>

<span class="sd">    Args:</span>
<span class="sd">        dictionary_vector_indices: Indices of the dictionary vectors to update.</span>
<span class="sd">        updated_weights: Updated weights for just these dictionary vectors.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">dictionary_vector_indices</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span>

    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">[</span><span class="n">dictionary_vector_indices</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">updated_weights</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">




<h2 id="sparse_autoencoder.autoencoder.AbstractEncoder" class="doc doc-heading">
          <code>AbstractEncoder</code>


</h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><span title="torch.nn.Module">Module</span></code>, <code><span title="abc.ABC">ABC</span></code></p>

  
      <p>Abstract encoder module.</p>
<p>Typically includes :attr:<code>weights</code> and :attr:<code>bias</code> parameters, as well as an activation
function.</p>

            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/autoencoder/components/abstract_encoder.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span>
<span class="normal">85</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">AbstractEncoder</span><span class="p">(</span><span class="n">Module</span><span class="p">,</span> <span class="n">ABC</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Abstract encoder module.</span>

<span class="sd">    Typically includes :attr:`weights` and :attr:`bias` parameters, as well as an activation</span>
<span class="sd">    function.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@property</span>
    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">weight</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">EncoderWeights</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Weight.&quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="nd">@property</span>
    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">bias</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">LearntActivationVector</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Bias.&quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">LearnedActivationBatch</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Forward pass.</span>

<span class="sd">        Args:</span>
<span class="sd">            x: Input activations.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Resulting activations.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="nd">@final</span>
    <span class="k">def</span> <span class="nf">update_dictionary_vectors</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">dictionary_vector_indices</span><span class="p">:</span> <span class="n">InputOutputNeuronIndices</span><span class="p">,</span>
        <span class="n">updated_dictionary_weights</span><span class="p">:</span> <span class="n">DeadEncoderNeuronWeightUpdates</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Update encoder dictionary vectors.</span>

<span class="sd">        Updates the dictionary vectors (columns in the weight matrix) with the given values.</span>

<span class="sd">        Args:</span>
<span class="sd">            dictionary_vector_indices: Indices of the dictionary vectors to update.</span>
<span class="sd">            updated_dictionary_weights: Updated weights for just these dictionary vectors.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">dictionary_vector_indices</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">[:,</span> <span class="n">dictionary_vector_indices</span><span class="p">]</span> <span class="o">=</span> <span class="n">updated_dictionary_weights</span>

    <span class="nd">@final</span>
    <span class="k">def</span> <span class="nf">update_bias</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">update_parameter_indices</span><span class="p">:</span> <span class="n">InputOutputNeuronIndices</span><span class="p">,</span>
        <span class="n">updated_bias_features</span><span class="p">:</span> <span class="n">LearntActivationVector</span> <span class="o">|</span> <span class="nb">float</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Update encoder bias.</span>

<span class="sd">        Args:</span>
<span class="sd">            update_parameter_indices: Indices of the bias features to update.</span>
<span class="sd">            updated_bias_features: Updated bias features for just these indices.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">update_parameter_indices</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">[</span><span class="n">update_parameter_indices</span><span class="p">]</span> <span class="o">=</span> <span class="n">updated_bias_features</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">




<h3 id="sparse_autoencoder.autoencoder.AbstractEncoder.bias" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">bias</span><span class="p">:</span> <span class="n">LearntActivationVector</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-abstractmethod"><code>abstractmethod</code></small>
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>Bias.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">




<h3 id="sparse_autoencoder.autoencoder.AbstractEncoder.weight" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">weight</span><span class="p">:</span> <span class="n">EncoderWeights</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-abstractmethod"><code>abstractmethod</code></small>
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>Weight.</p>
  </div>

</div>




<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.autoencoder.AbstractEncoder.forward" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-abstractmethod"><code>abstractmethod</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>Forward pass.</p>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>x</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.InputOutputActivationBatch" href="../tensor_types/#sparse_autoencoder.tensor_types.InputOutputActivationBatch">InputOutputActivationBatch</a></code>)
              –
              <div class="doc-md-description">
                <p>Input activations.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
    <th class="field-name">Returns:</th>
    <td class="field-body">
      <ul class="first simple">
          <li>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.LearnedActivationBatch" href="../tensor_types/#sparse_autoencoder.tensor_types.LearnedActivationBatch">LearnedActivationBatch</a></code>
            –
            <div class="doc-md-description">
              <p>Resulting activations.</p>
            </div>
          </li>
      </ul>
    </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary> <code>sparse_autoencoder/autoencoder/components/abstract_encoder.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@abstractmethod</span>
<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">LearnedActivationBatch</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Forward pass.</span>

<span class="sd">    Args:</span>
<span class="sd">        x: Input activations.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Resulting activations.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.autoencoder.AbstractEncoder.update_bias" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">update_bias</span><span class="p">(</span><span class="n">update_parameter_indices</span><span class="p">,</span> <span class="n">updated_bias_features</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Update encoder bias.</p>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>update_parameter_indices</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.InputOutputNeuronIndices" href="../tensor_types/#sparse_autoencoder.tensor_types.InputOutputNeuronIndices">InputOutputNeuronIndices</a></code>)
              –
              <div class="doc-md-description">
                <p>Indices of the bias features to update.</p>
              </div>
            </li>
            <li>
              <b><code>updated_bias_features</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.LearntActivationVector" href="../tensor_types/#sparse_autoencoder.tensor_types.LearntActivationVector">LearntActivationVector</a> | float</code>)
              –
              <div class="doc-md-description">
                <p>Updated bias features for just these indices.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary> <code>sparse_autoencoder/autoencoder/components/abstract_encoder.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span>
<span class="normal">85</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@final</span>
<span class="k">def</span> <span class="nf">update_bias</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">update_parameter_indices</span><span class="p">:</span> <span class="n">InputOutputNeuronIndices</span><span class="p">,</span>
    <span class="n">updated_bias_features</span><span class="p">:</span> <span class="n">LearntActivationVector</span> <span class="o">|</span> <span class="nb">float</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Update encoder bias.</span>

<span class="sd">    Args:</span>
<span class="sd">        update_parameter_indices: Indices of the bias features to update.</span>
<span class="sd">        updated_bias_features: Updated bias features for just these indices.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">update_parameter_indices</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span>

    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">[</span><span class="n">update_parameter_indices</span><span class="p">]</span> <span class="o">=</span> <span class="n">updated_bias_features</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.autoencoder.AbstractEncoder.update_dictionary_vectors" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">update_dictionary_vectors</span><span class="p">(</span><span class="n">dictionary_vector_indices</span><span class="p">,</span> <span class="n">updated_dictionary_weights</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Update encoder dictionary vectors.</p>
<p>Updates the dictionary vectors (columns in the weight matrix) with the given values.</p>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>dictionary_vector_indices</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.InputOutputNeuronIndices" href="../tensor_types/#sparse_autoencoder.tensor_types.InputOutputNeuronIndices">InputOutputNeuronIndices</a></code>)
              –
              <div class="doc-md-description">
                <p>Indices of the dictionary vectors to update.</p>
              </div>
            </li>
            <li>
              <b><code>updated_dictionary_weights</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.DeadEncoderNeuronWeightUpdates" href="../tensor_types/#sparse_autoencoder.tensor_types.DeadEncoderNeuronWeightUpdates">DeadEncoderNeuronWeightUpdates</a></code>)
              –
              <div class="doc-md-description">
                <p>Updated weights for just these dictionary vectors.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary> <code>sparse_autoencoder/autoencoder/components/abstract_encoder.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@final</span>
<span class="k">def</span> <span class="nf">update_dictionary_vectors</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">dictionary_vector_indices</span><span class="p">:</span> <span class="n">InputOutputNeuronIndices</span><span class="p">,</span>
    <span class="n">updated_dictionary_weights</span><span class="p">:</span> <span class="n">DeadEncoderNeuronWeightUpdates</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Update encoder dictionary vectors.</span>

<span class="sd">    Updates the dictionary vectors (columns in the weight matrix) with the given values.</span>

<span class="sd">    Args:</span>
<span class="sd">        dictionary_vector_indices: Indices of the dictionary vectors to update.</span>
<span class="sd">        updated_dictionary_weights: Updated weights for just these dictionary vectors.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">dictionary_vector_indices</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span>

    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">[:,</span> <span class="n">dictionary_vector_indices</span><span class="p">]</span> <span class="o">=</span> <span class="n">updated_dictionary_weights</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">




<h2 id="sparse_autoencoder.autoencoder.AbstractOuterBias" class="doc doc-heading">
          <code>AbstractOuterBias</code>


</h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><span title="torch.nn.Module">Module</span></code>, <code><span title="abc.ABC">ABC</span></code></p>

  
      <p>Abstract Pre-Encoder or Post-Decoder Bias Module.</p>

            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/autoencoder/components/abstract_outer_bias.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">AbstractOuterBias</span><span class="p">(</span><span class="n">Module</span><span class="p">,</span> <span class="n">ABC</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Abstract Pre-Encoder or Post-Decoder Bias Module.&quot;&quot;&quot;</span>

    <span class="nd">@property</span>
    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">bias</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">InputOutputActivationVector</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Bias.</span>

<span class="sd">        May be a reference to a bias parameter in the parent module, if using e.g. a tied bias.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">InputOutputActivationBatch</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Forward Pass.</span>

<span class="sd">        Args:</span>
<span class="sd">            x: Input activations (e.g. activations from an MLP layer in a transformer model).</span>

<span class="sd">        Returns:</span>
<span class="sd">            Resulting activations.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">




<h3 id="sparse_autoencoder.autoencoder.AbstractOuterBias.bias" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">bias</span><span class="p">:</span> <span class="n">InputOutputActivationVector</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-abstractmethod"><code>abstractmethod</code></small>
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>Bias.</p>
<p>May be a reference to a bias parameter in the parent module, if using e.g. a tied bias.</p>
  </div>

</div>




<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.autoencoder.AbstractOuterBias.forward" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-abstractmethod"><code>abstractmethod</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>Forward Pass.</p>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>x</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.InputOutputActivationBatch" href="../tensor_types/#sparse_autoencoder.tensor_types.InputOutputActivationBatch">InputOutputActivationBatch</a></code>)
              –
              <div class="doc-md-description">
                <p>Input activations (e.g. activations from an MLP layer in a transformer model).</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
    <th class="field-name">Returns:</th>
    <td class="field-body">
      <ul class="first simple">
          <li>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.InputOutputActivationBatch" href="../tensor_types/#sparse_autoencoder.tensor_types.InputOutputActivationBatch">InputOutputActivationBatch</a></code>
            –
            <div class="doc-md-description">
              <p>Resulting activations.</p>
            </div>
          </li>
      </ul>
    </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary> <code>sparse_autoencoder/autoencoder/components/abstract_outer_bias.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@abstractmethod</span>
<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">InputOutputActivationBatch</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Forward Pass.</span>

<span class="sd">    Args:</span>
<span class="sd">        x: Input activations (e.g. activations from an MLP layer in a transformer model).</span>

<span class="sd">    Returns:</span>
<span class="sd">        Resulting activations.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">




<h2 id="sparse_autoencoder.autoencoder.LinearEncoder" class="doc doc-heading">
          <code>LinearEncoder</code>


</h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.autoencoder.components.abstract_encoder.AbstractEncoder" href="components/abstract_encoder/#sparse_autoencoder.autoencoder.components.abstract_encoder.AbstractEncoder">AbstractEncoder</a></code></p>

  
      <p>Linear encoder layer.</p>

            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/autoencoder/components/linear_encoder.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@final</span>
<span class="k">class</span> <span class="nc">LinearEncoder</span><span class="p">(</span><span class="n">AbstractEncoder</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Linear encoder layer.&quot;&quot;&quot;</span>

    <span class="n">_learnt_features</span><span class="p">:</span> <span class="nb">int</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Number of learnt features (inputs to this layer).&quot;&quot;&quot;</span>

    <span class="n">_input_features</span><span class="p">:</span> <span class="nb">int</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Number of decoded features (outputs from this layer).&quot;&quot;&quot;</span>

    <span class="n">_weight</span><span class="p">:</span> <span class="n">EncoderWeights</span>

    <span class="n">_bias</span><span class="p">:</span> <span class="n">LearntActivationVector</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">weight</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">EncoderWeights</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Weight.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_weight</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">bias</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">LearntActivationVector</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Bias.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_bias</span>

    <span class="n">activation_function</span><span class="p">:</span> <span class="n">ReLU</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">learnt_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize the linear encoder layer.&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_learnt_features</span> <span class="o">=</span> <span class="n">learnt_features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_input_features</span> <span class="o">=</span> <span class="n">input_features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation_function</span> <span class="o">=</span> <span class="n">ReLU</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
                <span class="p">(</span><span class="n">learnt_features</span><span class="p">,</span> <span class="n">input_features</span><span class="p">),</span>
            <span class="p">)</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_bias</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">learnt_features</span><span class="p">))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">reset_parameters</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize or reset the parameters.&quot;&quot;&quot;</span>
        <span class="c1"># Setting a=sqrt(5) in kaiming_uniform is the same as initializing with</span>
        <span class="c1"># uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see</span>
        <span class="c1"># https://github.com/pytorch/pytorch/issues/57109</span>
        <span class="n">init</span><span class="o">.</span><span class="n">kaiming_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_weight</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>

        <span class="c1"># Bias (approach from nn.Linear)</span>
        <span class="n">fan_in</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_weight</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">bound</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">fan_in</span><span class="p">)</span> <span class="k">if</span> <span class="n">fan_in</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">0</span>
        <span class="n">init</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_bias</span><span class="p">,</span> <span class="o">-</span><span class="n">bound</span><span class="p">,</span> <span class="n">bound</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">LearnedActivationBatch</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Forward pass.</span>

<span class="sd">        Args:</span>
<span class="sd">            x: Input tensor.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Output of the forward pass.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">learned_activation_batch</span><span class="p">:</span> <span class="n">LearnedActivationBatch</span> <span class="o">=</span> <span class="n">einops</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span>
            <span class="n">x</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
            <span class="s2">&quot;batch input_output_feature, </span><span class="se">\</span>
<span class="s2">                learnt_feature_dim input_output_feature_dim </span><span class="se">\</span>
<span class="s2">                -&gt; batch learnt_feature_dim&quot;</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">learned_activation_batch</span> <span class="o">=</span> <span class="n">einops</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span>
            <span class="n">learned_activation_batch</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span>
            <span class="s2">&quot;batch learnt_feature_dim, </span><span class="se">\</span>
<span class="s2">                learnt_feature_dim -&gt; batch learnt_feature_dim&quot;</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation_function</span><span class="p">(</span><span class="n">learned_activation_batch</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;String extra representation of the module.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;in_features=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_input_features</span><span class="si">}</span><span class="s2">, out_features=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_learnt_features</span><span class="si">}</span><span class="s2">&quot;</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">




<h3 id="sparse_autoencoder.autoencoder.LinearEncoder.bias" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">bias</span><span class="p">:</span> <span class="n">LearntActivationVector</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>Bias.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">




<h3 id="sparse_autoencoder.autoencoder.LinearEncoder.weight" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">weight</span><span class="p">:</span> <span class="n">EncoderWeights</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>Weight.</p>
  </div>

</div>




<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.autoencoder.LinearEncoder.__init__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">input_features</span><span class="p">,</span> <span class="n">learnt_features</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Initialize the linear encoder layer.</p>

          <details class="quote">
            <summary> <code>sparse_autoencoder/autoencoder/components/linear_encoder.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">input_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">learnt_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Initialize the linear encoder layer.&quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_learnt_features</span> <span class="o">=</span> <span class="n">learnt_features</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_input_features</span> <span class="o">=</span> <span class="n">input_features</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">activation_function</span> <span class="o">=</span> <span class="n">ReLU</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
            <span class="p">(</span><span class="n">learnt_features</span><span class="p">,</span> <span class="n">input_features</span><span class="p">),</span>
        <span class="p">)</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_bias</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">learnt_features</span><span class="p">))</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">reset_parameters</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.autoencoder.LinearEncoder.extra_repr" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">extra_repr</span><span class="p">()</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>String extra representation of the module.</p>

          <details class="quote">
            <summary> <code>sparse_autoencoder/autoencoder/components/linear_encoder.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;String extra representation of the module.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;in_features=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_input_features</span><span class="si">}</span><span class="s2">, out_features=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_learnt_features</span><span class="si">}</span><span class="s2">&quot;</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.autoencoder.LinearEncoder.forward" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Forward pass.</p>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>x</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.InputOutputActivationBatch" href="../tensor_types/#sparse_autoencoder.tensor_types.InputOutputActivationBatch">InputOutputActivationBatch</a></code>)
              –
              <div class="doc-md-description">
                <p>Input tensor.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
    <th class="field-name">Returns:</th>
    <td class="field-body">
      <ul class="first simple">
          <li>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.LearnedActivationBatch" href="../tensor_types/#sparse_autoencoder.tensor_types.LearnedActivationBatch">LearnedActivationBatch</a></code>
            –
            <div class="doc-md-description">
              <p>Output of the forward pass.</p>
            </div>
          </li>
      </ul>
    </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary> <code>sparse_autoencoder/autoencoder/components/linear_encoder.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">LearnedActivationBatch</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Forward pass.</span>

<span class="sd">    Args:</span>
<span class="sd">        x: Input tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Output of the forward pass.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">learned_activation_batch</span><span class="p">:</span> <span class="n">LearnedActivationBatch</span> <span class="o">=</span> <span class="n">einops</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span>
        <span class="n">x</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
        <span class="s2">&quot;batch input_output_feature, </span><span class="se">\</span>
<span class="s2">            learnt_feature_dim input_output_feature_dim </span><span class="se">\</span>
<span class="s2">            -&gt; batch learnt_feature_dim&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">learned_activation_batch</span> <span class="o">=</span> <span class="n">einops</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span>
        <span class="n">learned_activation_batch</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span>
        <span class="s2">&quot;batch learnt_feature_dim, </span><span class="se">\</span>
<span class="s2">            learnt_feature_dim -&gt; batch learnt_feature_dim&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation_function</span><span class="p">(</span><span class="n">learned_activation_batch</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.autoencoder.LinearEncoder.reset_parameters" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">reset_parameters</span><span class="p">()</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Initialize or reset the parameters.</p>

          <details class="quote">
            <summary> <code>sparse_autoencoder/autoencoder/components/linear_encoder.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Initialize or reset the parameters.&quot;&quot;&quot;</span>
    <span class="c1"># Setting a=sqrt(5) in kaiming_uniform is the same as initializing with</span>
    <span class="c1"># uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see</span>
    <span class="c1"># https://github.com/pytorch/pytorch/issues/57109</span>
    <span class="n">init</span><span class="o">.</span><span class="n">kaiming_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_weight</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>

    <span class="c1"># Bias (approach from nn.Linear)</span>
    <span class="n">fan_in</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_weight</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">bound</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">fan_in</span><span class="p">)</span> <span class="k">if</span> <span class="n">fan_in</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">0</span>
    <span class="n">init</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_bias</span><span class="p">,</span> <span class="o">-</span><span class="n">bound</span><span class="p">,</span> <span class="n">bound</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">




<h2 id="sparse_autoencoder.autoencoder.SparseAutoencoder" class="doc doc-heading">
          <code>SparseAutoencoder</code>


</h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.autoencoder.abstract_autoencoder.AbstractAutoencoder" href="abstract_autoencoder/#sparse_autoencoder.autoencoder.abstract_autoencoder.AbstractAutoencoder">AbstractAutoencoder</a></code></p>

  
      <p>Sparse Autoencoder Model.</p>

            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/autoencoder/model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@final</span>
<span class="k">class</span> <span class="nc">SparseAutoencoder</span><span class="p">(</span><span class="n">AbstractAutoencoder</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Sparse Autoencoder Model.&quot;&quot;&quot;</span>

    <span class="n">geometric_median_dataset</span><span class="p">:</span> <span class="n">InputOutputActivationVector</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Estimated Geometric Median of the Dataset.</span>

<span class="sd">    Used for initialising :attr:`tied_bias`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">tied_bias</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Tied Bias Parameter.</span>

<span class="sd">    The same bias is used pre-encoder and post-decoder.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">n_input_features</span><span class="p">:</span> <span class="nb">int</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Number of Input Features.&quot;&quot;&quot;</span>

    <span class="n">n_learned_features</span><span class="p">:</span> <span class="nb">int</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Number of Learned Features.&quot;&quot;&quot;</span>

    <span class="n">_pre_encoder_bias</span><span class="p">:</span> <span class="n">TiedBias</span>

    <span class="n">_encoder</span><span class="p">:</span> <span class="n">LinearEncoder</span>

    <span class="n">_decoder</span><span class="p">:</span> <span class="n">UnitNormDecoder</span>

    <span class="n">_post_decoder_bias</span><span class="p">:</span> <span class="n">TiedBias</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">pre_encoder_bias</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TiedBias</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Pre-encoder bias.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pre_encoder_bias</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">encoder</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">LinearEncoder</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Encoder.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encoder</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">decoder</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">UnitNormDecoder</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Decoder.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_decoder</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">post_decoder_bias</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TiedBias</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Post-decoder bias.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_post_decoder_bias</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">n_input_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">n_learned_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">geometric_median_dataset</span><span class="p">:</span> <span class="n">InputOutputActivationVector</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize the Sparse Autoencoder Model.</span>

<span class="sd">        Args:</span>
<span class="sd">            n_input_features: Number of input features (e.g. `d_mlp` if training on MLP activations</span>
<span class="sd">                from TransformerLens).</span>
<span class="sd">            n_learned_features: Number of learned features. The initial paper experimented with 1 to</span>
<span class="sd">                256 times the number of input features, and primarily used a multiple of 8.</span>
<span class="sd">            geometric_median_dataset: Estimated geometric median of the dataset.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">n_input_features</span> <span class="o">=</span> <span class="n">n_input_features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_learned_features</span> <span class="o">=</span> <span class="n">n_learned_features</span>

        <span class="c1"># Store the geometric median of the dataset (so that we can reset parameters). This is not a</span>
        <span class="c1"># parameter itself (the tied bias parameter is used for that), so gradients are disabled.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">geometric_median_dataset</span> <span class="o">=</span> <span class="n">geometric_median_dataset</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">geometric_median_dataset</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="c1"># Initialize the tied bias</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tied_bias</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">n_input_features</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">initialize_tied_parameters</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_pre_encoder_bias</span> <span class="o">=</span> <span class="n">TiedBias</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tied_bias</span><span class="p">,</span> <span class="n">TiedBiasPosition</span><span class="o">.</span><span class="n">PRE_ENCODER</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_encoder</span> <span class="o">=</span> <span class="n">LinearEncoder</span><span class="p">(</span>
            <span class="n">input_features</span><span class="o">=</span><span class="n">n_input_features</span><span class="p">,</span> <span class="n">learnt_features</span><span class="o">=</span><span class="n">n_learned_features</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_decoder</span> <span class="o">=</span> <span class="n">UnitNormDecoder</span><span class="p">(</span>
            <span class="n">learnt_features</span><span class="o">=</span><span class="n">n_learned_features</span><span class="p">,</span> <span class="n">decoded_features</span><span class="o">=</span><span class="n">n_input_features</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_post_decoder_bias</span> <span class="o">=</span> <span class="n">TiedBias</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tied_bias</span><span class="p">,</span> <span class="n">TiedBiasPosition</span><span class="o">.</span><span class="n">POST_DECODER</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span>
        <span class="n">LearnedActivationBatch</span><span class="p">,</span>
        <span class="n">InputOutputActivationBatch</span><span class="p">,</span>
    <span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Forward Pass.</span>

<span class="sd">        Args:</span>
<span class="sd">            x: Input activations (e.g. activations from an MLP layer in a transformer model).</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tuple of learned activations and decoded activations.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pre_encoder_bias</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">learned_activations</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_decoder</span><span class="p">(</span><span class="n">learned_activations</span><span class="p">)</span>
        <span class="n">decoded_activations</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_post_decoder_bias</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">learned_activations</span><span class="p">,</span> <span class="n">decoded_activations</span>

    <span class="k">def</span> <span class="nf">initialize_tied_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize the tied parameters.&quot;&quot;&quot;</span>
        <span class="c1"># The tied bias is initialised as the geometric median of the dataset</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tied_bias</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">geometric_median_dataset</span>

    <span class="k">def</span> <span class="nf">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Reset the parameters.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">initialize_tied_parameters</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">:</span>
            <span class="k">if</span> <span class="s2">&quot;reset_parameters&quot;</span> <span class="ow">in</span> <span class="nb">dir</span><span class="p">(</span><span class="n">module</span><span class="p">):</span>
                <span class="n">module</span><span class="o">.</span><span class="n">reset_parameters</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">




<h3 id="sparse_autoencoder.autoencoder.SparseAutoencoder.decoder" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">decoder</span><span class="p">:</span> <span class="n">UnitNormDecoder</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>Decoder.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">




<h3 id="sparse_autoencoder.autoencoder.SparseAutoencoder.encoder" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">encoder</span><span class="p">:</span> <span class="n">LinearEncoder</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>Encoder.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">




<h3 id="sparse_autoencoder.autoencoder.SparseAutoencoder.geometric_median_dataset" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">geometric_median_dataset</span><span class="p">:</span> <span class="n">InputOutputActivationVector</span> <span class="o">=</span> <span class="n">geometric_median_dataset</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>Estimated Geometric Median of the Dataset.</p>
<p>Used for initialising :attr:<code>tied_bias</code>.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">




<h3 id="sparse_autoencoder.autoencoder.SparseAutoencoder.n_input_features" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">n_input_features</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">n_input_features</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>Number of Input Features.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">




<h3 id="sparse_autoencoder.autoencoder.SparseAutoencoder.n_learned_features" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">n_learned_features</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">n_learned_features</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>Number of Learned Features.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">




<h3 id="sparse_autoencoder.autoencoder.SparseAutoencoder.post_decoder_bias" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">post_decoder_bias</span><span class="p">:</span> <span class="n">TiedBias</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>Post-decoder bias.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">




<h3 id="sparse_autoencoder.autoencoder.SparseAutoencoder.pre_encoder_bias" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">pre_encoder_bias</span><span class="p">:</span> <span class="n">TiedBias</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>Pre-encoder bias.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">




<h3 id="sparse_autoencoder.autoencoder.SparseAutoencoder.tied_bias" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">tied_bias</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">n_input_features</span><span class="p">))</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>Tied Bias Parameter.</p>
<p>The same bias is used pre-encoder and post-decoder.</p>
  </div>

</div>




<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.autoencoder.SparseAutoencoder.__init__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">n_input_features</span><span class="p">,</span> <span class="n">n_learned_features</span><span class="p">,</span> <span class="n">geometric_median_dataset</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Initialize the Sparse Autoencoder Model.</p>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>n_input_features</code></b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>Number of input features (e.g. <code>d_mlp</code> if training on MLP activations
from TransformerLens).</p>
              </div>
            </li>
            <li>
              <b><code>n_learned_features</code></b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>Number of learned features. The initial paper experimented with 1 to
256 times the number of input features, and primarily used a multiple of 8.</p>
              </div>
            </li>
            <li>
              <b><code>geometric_median_dataset</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.InputOutputActivationVector" href="../tensor_types/#sparse_autoencoder.tensor_types.InputOutputActivationVector">InputOutputActivationVector</a></code>)
              –
              <div class="doc-md-description">
                <p>Estimated geometric median of the dataset.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary> <code>sparse_autoencoder/autoencoder/model.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">n_input_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">n_learned_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">geometric_median_dataset</span><span class="p">:</span> <span class="n">InputOutputActivationVector</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Initialize the Sparse Autoencoder Model.</span>

<span class="sd">    Args:</span>
<span class="sd">        n_input_features: Number of input features (e.g. `d_mlp` if training on MLP activations</span>
<span class="sd">            from TransformerLens).</span>
<span class="sd">        n_learned_features: Number of learned features. The initial paper experimented with 1 to</span>
<span class="sd">            256 times the number of input features, and primarily used a multiple of 8.</span>
<span class="sd">        geometric_median_dataset: Estimated geometric median of the dataset.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">n_input_features</span> <span class="o">=</span> <span class="n">n_input_features</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_learned_features</span> <span class="o">=</span> <span class="n">n_learned_features</span>

    <span class="c1"># Store the geometric median of the dataset (so that we can reset parameters). This is not a</span>
    <span class="c1"># parameter itself (the tied bias parameter is used for that), so gradients are disabled.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">geometric_median_dataset</span> <span class="o">=</span> <span class="n">geometric_median_dataset</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">geometric_median_dataset</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="c1"># Initialize the tied bias</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">tied_bias</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">n_input_features</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">initialize_tied_parameters</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_pre_encoder_bias</span> <span class="o">=</span> <span class="n">TiedBias</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tied_bias</span><span class="p">,</span> <span class="n">TiedBiasPosition</span><span class="o">.</span><span class="n">PRE_ENCODER</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_encoder</span> <span class="o">=</span> <span class="n">LinearEncoder</span><span class="p">(</span>
        <span class="n">input_features</span><span class="o">=</span><span class="n">n_input_features</span><span class="p">,</span> <span class="n">learnt_features</span><span class="o">=</span><span class="n">n_learned_features</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_decoder</span> <span class="o">=</span> <span class="n">UnitNormDecoder</span><span class="p">(</span>
        <span class="n">learnt_features</span><span class="o">=</span><span class="n">n_learned_features</span><span class="p">,</span> <span class="n">decoded_features</span><span class="o">=</span><span class="n">n_input_features</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_post_decoder_bias</span> <span class="o">=</span> <span class="n">TiedBias</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tied_bias</span><span class="p">,</span> <span class="n">TiedBiasPosition</span><span class="o">.</span><span class="n">POST_DECODER</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.autoencoder.SparseAutoencoder.forward" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Forward Pass.</p>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>x</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.InputOutputActivationBatch" href="../tensor_types/#sparse_autoencoder.tensor_types.InputOutputActivationBatch">InputOutputActivationBatch</a></code>)
              –
              <div class="doc-md-description">
                <p>Input activations (e.g. activations from an MLP layer in a transformer model).</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
    <th class="field-name">Returns:</th>
    <td class="field-body">
      <ul class="first simple">
          <li>
                <code>tuple[<a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.LearnedActivationBatch" href="../tensor_types/#sparse_autoencoder.tensor_types.LearnedActivationBatch">LearnedActivationBatch</a>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.InputOutputActivationBatch" href="../tensor_types/#sparse_autoencoder.tensor_types.InputOutputActivationBatch">InputOutputActivationBatch</a>]</code>
            –
            <div class="doc-md-description">
              <p>Tuple of learned activations and decoded activations.</p>
            </div>
          </li>
      </ul>
    </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary> <code>sparse_autoencoder/autoencoder/model.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span>
    <span class="n">LearnedActivationBatch</span><span class="p">,</span>
    <span class="n">InputOutputActivationBatch</span><span class="p">,</span>
<span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Forward Pass.</span>

<span class="sd">    Args:</span>
<span class="sd">        x: Input activations (e.g. activations from an MLP layer in a transformer model).</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tuple of learned activations and decoded activations.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pre_encoder_bias</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">learned_activations</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_decoder</span><span class="p">(</span><span class="n">learned_activations</span><span class="p">)</span>
    <span class="n">decoded_activations</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_post_decoder_bias</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">learned_activations</span><span class="p">,</span> <span class="n">decoded_activations</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.autoencoder.SparseAutoencoder.initialize_tied_parameters" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">initialize_tied_parameters</span><span class="p">()</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Initialize the tied parameters.</p>

          <details class="quote">
            <summary> <code>sparse_autoencoder/autoencoder/model.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">initialize_tied_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Initialize the tied parameters.&quot;&quot;&quot;</span>
    <span class="c1"># The tied bias is initialised as the geometric median of the dataset</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">tied_bias</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">geometric_median_dataset</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.autoencoder.SparseAutoencoder.reset_parameters" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">reset_parameters</span><span class="p">()</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Reset the parameters.</p>

          <details class="quote">
            <summary> <code>sparse_autoencoder/autoencoder/model.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Reset the parameters.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">initialize_tied_parameters</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">:</span>
        <span class="k">if</span> <span class="s2">&quot;reset_parameters&quot;</span> <span class="ow">in</span> <span class="nb">dir</span><span class="p">(</span><span class="n">module</span><span class="p">):</span>
            <span class="n">module</span><span class="o">.</span><span class="n">reset_parameters</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">




<h2 id="sparse_autoencoder.autoencoder.TiedBias" class="doc doc-heading">
          <code>TiedBias</code>


</h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.autoencoder.components.abstract_outer_bias.AbstractOuterBias" href="components/abstract_outer_bias/#sparse_autoencoder.autoencoder.components.abstract_outer_bias.AbstractOuterBias">AbstractOuterBias</a></code></p>

  
      <p>Tied Bias Layer.</p>
<p>The tied pre-encoder bias is a learned bias term that is subtracted from the input before
encoding, and added back after decoding.</p>
<p>The bias parameter must be initialised in the parent module, and then passed to this layer.</p>
<p>https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-bias</p>

            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/autoencoder/components/tied_bias.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@final</span>
<span class="k">class</span> <span class="nc">TiedBias</span><span class="p">(</span><span class="n">AbstractOuterBias</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Tied Bias Layer.</span>

<span class="sd">    The tied pre-encoder bias is a learned bias term that is subtracted from the input before</span>
<span class="sd">    encoding, and added back after decoding.</span>

<span class="sd">    The bias parameter must be initialised in the parent module, and then passed to this layer.</span>

<span class="sd">    https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-bias</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_bias_position</span><span class="p">:</span> <span class="n">TiedBiasPosition</span>

    <span class="n">_bias_reference</span><span class="p">:</span> <span class="n">InputOutputActivationVector</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">bias</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">InputOutputActivationVector</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Bias.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_bias_reference</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">bias_reference</span><span class="p">:</span> <span class="n">InputOutputActivationVector</span><span class="p">,</span>
        <span class="n">position</span><span class="p">:</span> <span class="n">TiedBiasPosition</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize the bias layer.</span>

<span class="sd">        Args:</span>
<span class="sd">            bias_reference: Tied bias parameter (initialised in the parent module), used for both</span>
<span class="sd">                the pre-encoder and post-encoder bias. The original paper initialised this using the</span>
<span class="sd">                geometric median of the dataset.</span>
<span class="sd">            position: Whether this is the pre-encoder or post-encoder bias.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_bias_reference</span> <span class="o">=</span> <span class="n">bias_reference</span>

        <span class="c1"># Support string literals as well as enums</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_bias_position</span> <span class="o">=</span> <span class="n">position</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">InputOutputActivationBatch</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Forward Pass.</span>

<span class="sd">        Args:</span>
<span class="sd">            x: Input tensor.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Output of the forward pass.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># If this is the pre-encoder bias, we subtract the bias from the input.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_bias_position</span> <span class="o">==</span> <span class="n">TiedBiasPosition</span><span class="o">.</span><span class="n">PRE_ENCODER</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">x</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>

        <span class="c1"># If it&#39;s the post-encoder bias, we add the bias to the input.</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>

    <span class="k">def</span> <span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;String extra representation of the module.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;position=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_bias_position</span><span class="o">.</span><span class="n">value</span><span class="si">}</span><span class="s2">&quot;</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">




<h3 id="sparse_autoencoder.autoencoder.TiedBias.bias" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">bias</span><span class="p">:</span> <span class="n">InputOutputActivationVector</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>Bias.</p>
  </div>

</div>




<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.autoencoder.TiedBias.__init__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">bias_reference</span><span class="p">,</span> <span class="n">position</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Initialize the bias layer.</p>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>bias_reference</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.InputOutputActivationVector" href="../tensor_types/#sparse_autoencoder.tensor_types.InputOutputActivationVector">InputOutputActivationVector</a></code>)
              –
              <div class="doc-md-description">
                <p>Tied bias parameter (initialised in the parent module), used for both
the pre-encoder and post-encoder bias. The original paper initialised this using the
geometric median of the dataset.</p>
              </div>
            </li>
            <li>
              <b><code>position</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.autoencoder.components.tied_bias.TiedBiasPosition" href="components/tied_bias/#sparse_autoencoder.autoencoder.components.tied_bias.TiedBiasPosition">TiedBiasPosition</a></code>)
              –
              <div class="doc-md-description">
                <p>Whether this is the pre-encoder or post-encoder bias.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary> <code>sparse_autoencoder/autoencoder/components/tied_bias.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">bias_reference</span><span class="p">:</span> <span class="n">InputOutputActivationVector</span><span class="p">,</span>
    <span class="n">position</span><span class="p">:</span> <span class="n">TiedBiasPosition</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Initialize the bias layer.</span>

<span class="sd">    Args:</span>
<span class="sd">        bias_reference: Tied bias parameter (initialised in the parent module), used for both</span>
<span class="sd">            the pre-encoder and post-encoder bias. The original paper initialised this using the</span>
<span class="sd">            geometric median of the dataset.</span>
<span class="sd">        position: Whether this is the pre-encoder or post-encoder bias.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_bias_reference</span> <span class="o">=</span> <span class="n">bias_reference</span>

    <span class="c1"># Support string literals as well as enums</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_bias_position</span> <span class="o">=</span> <span class="n">position</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.autoencoder.TiedBias.extra_repr" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">extra_repr</span><span class="p">()</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>String extra representation of the module.</p>

          <details class="quote">
            <summary> <code>sparse_autoencoder/autoencoder/components/tied_bias.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;String extra representation of the module.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;position=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_bias_position</span><span class="o">.</span><span class="n">value</span><span class="si">}</span><span class="s2">&quot;</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.autoencoder.TiedBias.forward" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Forward Pass.</p>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>x</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.InputOutputActivationBatch" href="../tensor_types/#sparse_autoencoder.tensor_types.InputOutputActivationBatch">InputOutputActivationBatch</a></code>)
              –
              <div class="doc-md-description">
                <p>Input tensor.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
    <th class="field-name">Returns:</th>
    <td class="field-body">
      <ul class="first simple">
          <li>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.InputOutputActivationBatch" href="../tensor_types/#sparse_autoencoder.tensor_types.InputOutputActivationBatch">InputOutputActivationBatch</a></code>
            –
            <div class="doc-md-description">
              <p>Output of the forward pass.</p>
            </div>
          </li>
      </ul>
    </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary> <code>sparse_autoencoder/autoencoder/components/tied_bias.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">InputOutputActivationBatch</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Forward Pass.</span>

<span class="sd">    Args:</span>
<span class="sd">        x: Input tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Output of the forward pass.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># If this is the pre-encoder bias, we subtract the bias from the input.</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_bias_position</span> <span class="o">==</span> <span class="n">TiedBiasPosition</span><span class="o">.</span><span class="n">PRE_ENCODER</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>

    <span class="c1"># If it&#39;s the post-encoder bias, we add the bias to the input.</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">




<h2 id="sparse_autoencoder.autoencoder.TiedBiasPosition" class="doc doc-heading">
          <code>TiedBiasPosition</code>


</h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code>str</code>, <code><span title="enum.Enum">Enum</span></code></p>

  
      <p>Tied Bias Position.</p>

            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/autoencoder/components/tied_bias.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">TiedBiasPosition</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">Enum</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Tied Bias Position.&quot;&quot;&quot;</span>

    <span class="n">PRE_ENCODER</span> <span class="o">=</span> <span class="s2">&quot;pre_encoder&quot;</span>
    <span class="n">POST_DECODER</span> <span class="o">=</span> <span class="s2">&quot;post_decoder&quot;</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">











  </div>

  </div>

</div>

<div class="doc doc-object doc-class">




<h2 id="sparse_autoencoder.autoencoder.UnitNormDecoder" class="doc doc-heading">
          <code>UnitNormDecoder</code>


</h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.autoencoder.components.abstract_decoder.AbstractDecoder" href="components/abstract_decoder/#sparse_autoencoder.autoencoder.components.abstract_decoder.AbstractDecoder">AbstractDecoder</a></code></p>

  
      <p>Constrained unit norm linear decoder layer.</p>
<p>Linear layer decoder, where the dictionary vectors (rows of the weight matrix) are constrained
to have unit norm. This is done by removing the gradient information parallel to the dictionary
vectors before applying the gradient step, using a backward hook.</p>

<details class="motivation" open>
  <summary>Motivation</summary>
  <p>Unit norming the dictionary vectors, which are essentially the rows of the decoding
    matrices, serves a few purposes:</p>
<pre><code>1. It helps with numerical stability, by preventing the dictionary vectors from growing
    too large.
2. It acts as a form of regularization, preventing overfitting by not allowing any one
    feature to dominate the representation. It limits the capacity of the model by
    forcing the dictionary vectors to live on the hypersphere of radius 1.
3. It encourages sparsity. Since the dictionary vectors have a fixed length, the model
    must carefully select which features to activate in order to best reconstruct the
    input.
</code></pre>
<p>Note that the <em>Towards Monosemanticity: Decomposing Language Models With Dictionary
Learning</em> paper found that removing the gradient information parallel to the dictionary
vectors before applying the gradient step, rather than resetting the dictionary vectors to
unit norm after each gradient step, results in a small but real reduction in total
loss](https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-optimization).</p>
</details>
            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/autoencoder/components/unit_norm_decoder.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@final</span>
<span class="k">class</span> <span class="nc">UnitNormDecoder</span><span class="p">(</span><span class="n">AbstractDecoder</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Constrained unit norm linear decoder layer.</span>

<span class="sd">    Linear layer decoder, where the dictionary vectors (rows of the weight matrix) are constrained</span>
<span class="sd">    to have unit norm. This is done by removing the gradient information parallel to the dictionary</span>
<span class="sd">    vectors before applying the gradient step, using a backward hook.</span>

<span class="sd">    Motivation:</span>
<span class="sd">        Unit norming the dictionary vectors, which are essentially the rows of the decoding</span>
<span class="sd">            matrices, serves a few purposes:</span>

<span class="sd">            1. It helps with numerical stability, by preventing the dictionary vectors from growing</span>
<span class="sd">                too large.</span>
<span class="sd">            2. It acts as a form of regularization, preventing overfitting by not allowing any one</span>
<span class="sd">                feature to dominate the representation. It limits the capacity of the model by</span>
<span class="sd">                forcing the dictionary vectors to live on the hypersphere of radius 1.</span>
<span class="sd">            3. It encourages sparsity. Since the dictionary vectors have a fixed length, the model</span>
<span class="sd">                must carefully select which features to activate in order to best reconstruct the</span>
<span class="sd">                input.</span>

<span class="sd">        Note that the *Towards Monosemanticity: Decomposing Language Models With Dictionary</span>
<span class="sd">        Learning* paper found that removing the gradient information parallel to the dictionary</span>
<span class="sd">        vectors before applying the gradient step, rather than resetting the dictionary vectors to</span>
<span class="sd">        unit norm after each gradient step, results in a small but real reduction in total</span>
<span class="sd">        loss](https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-optimization).</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_learnt_features</span><span class="p">:</span> <span class="nb">int</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Number of learnt features (inputs to this layer).&quot;&quot;&quot;</span>

    <span class="n">_decoded_features</span><span class="p">:</span> <span class="nb">int</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Number of decoded features (outputs from this layer).&quot;&quot;&quot;</span>

    <span class="n">_weight</span><span class="p">:</span> <span class="n">DecoderWeights</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Weight parameter.&quot;&quot;&quot;</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">weight</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">DecoderWeights</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Weight.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_weight</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">learnt_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">decoded_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">enable_gradient_hook</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize the constrained unit norm linear layer.</span>

<span class="sd">        Args:</span>
<span class="sd">            learnt_features: Number of learnt features in the autoencoder.</span>
<span class="sd">            decoded_features: Number of decoded (output) features in the autoencoder.</span>
<span class="sd">            enable_gradient_hook: Enable the gradient backwards hook (modify the gradient before</span>
<span class="sd">                applying the gradient step, to maintain unit norm of the dictionary vectors).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Create the linear layer as per the standard PyTorch linear layer</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_learnt_features</span> <span class="o">=</span> <span class="n">learnt_features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_decoded_features</span> <span class="o">=</span> <span class="n">decoded_features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
                <span class="p">(</span><span class="n">decoded_features</span><span class="p">,</span> <span class="n">learnt_features</span><span class="p">),</span>
            <span class="p">)</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reset_parameters</span><span class="p">()</span>

        <span class="c1"># Register backward hook to remove any gradient information parallel to the dictionary</span>
        <span class="c1"># vectors (rows of the weight matrix) before applying the gradient step.</span>
        <span class="k">if</span> <span class="n">enable_gradient_hook</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_weight</span><span class="o">.</span><span class="n">register_hook</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_weight_backward_hook</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize or reset the parameters.</span>

<span class="sd">        Example:</span>
<span class="sd">            &gt;&gt;&gt; import torch</span>
<span class="sd">            &gt;&gt;&gt; # Create a layer with 4 columns (learnt features) and 3 rows (decoded features)</span>
<span class="sd">            &gt;&gt;&gt; layer = UnitNormDecoder(learnt_features=4, decoded_features=3)</span>
<span class="sd">            &gt;&gt;&gt; layer.reset_parameters()</span>
<span class="sd">            &gt;&gt;&gt; # Get the norm across the rows (by summing across the columns)</span>
<span class="sd">            &gt;&gt;&gt; row_norms = torch.sum(layer.weight ** 2, dim=1)</span>
<span class="sd">            &gt;&gt;&gt; row_norms.round(decimals=3).tolist()</span>
<span class="sd">            [1.0, 1.0, 1.0]</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Initialize the weights with a normal distribution. Note we don&#39;t use e.g. kaiming</span>
        <span class="c1"># normalisation here, since we immediately scale the weights to have unit norm (so the</span>
        <span class="c1"># initial standard deviation doesn&#39;t matter). Note also that `init.normal_` is in place.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_weight</span><span class="p">:</span> <span class="n">EncoderWeights</span> <span class="o">=</span> <span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_weight</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Scale so that each row has unit norm</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_weight</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_weight</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_weight_backward_hook</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">grad</span><span class="p">:</span> <span class="n">EncoderWeights</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">EncoderWeights</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Unit norm backward hook.</span>

<span class="sd">        By subtracting the projection of the gradient onto the dictionary vectors, we remove the</span>
<span class="sd">        component of the gradient that is parallel to the dictionary vectors and just keep the</span>
<span class="sd">        component that is orthogonal to the dictionary vectors (i.e. moving around the hypersphere).</span>
<span class="sd">        The result is that the backward pass does not change the norm of the dictionary vectors.</span>

<span class="sd">        Args:</span>
<span class="sd">            grad: Gradient with respect to the weights.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Project the gradients onto the dictionary vectors. Intuitively the dictionary vectors can</span>
        <span class="c1"># be thought of as vectors that end on the circumference of a hypersphere. The projection of</span>
        <span class="c1"># the gradient onto the dictionary vectors is the component of the gradient that is parallel</span>
        <span class="c1"># to the dictionary vectors, i.e. the component that moves to or from the center of the</span>
        <span class="c1"># hypersphere.</span>
        <span class="n">normalized_weight</span><span class="p">:</span> <span class="n">EncoderWeights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_weight</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_weight</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>

        <span class="c1"># Calculate the dot product of the gradients with the dictionary vectors.</span>
        <span class="c1"># This represents the component of the gradient parallel to each dictionary vector.</span>
        <span class="c1"># The result will be a tensor of shape [decoded_features].</span>
        <span class="n">dot_product</span> <span class="o">=</span> <span class="n">einops</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span>
            <span class="n">grad</span><span class="p">,</span>
            <span class="n">normalized_weight</span><span class="p">,</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">Axis</span><span class="o">.</span><span class="n">LEARNT_FEATURE</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">Axis</span><span class="o">.</span><span class="n">INPUT_OUTPUT_FEATURE</span><span class="si">}</span><span class="s2">, </span><span class="se">\</span>
<span class="s2">                </span><span class="si">{</span><span class="n">Axis</span><span class="o">.</span><span class="n">LEARNT_FEATURE</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">Axis</span><span class="o">.</span><span class="n">INPUT_OUTPUT_FEATURE</span><span class="si">}</span><span class="s2"> </span><span class="se">\</span>
<span class="s2">                -&gt; </span><span class="si">{</span><span class="n">Axis</span><span class="o">.</span><span class="n">LEARNT_FEATURE</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Scale the normalized weights by the dot product to get the projection.</span>
        <span class="c1"># The result will be of the same shape as &#39;grad&#39; and &#39;self.weight&#39;.</span>
        <span class="n">projection</span> <span class="o">=</span> <span class="n">einops</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span>
            <span class="n">dot_product</span><span class="p">,</span>
            <span class="n">normalized_weight</span><span class="p">,</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">Axis</span><span class="o">.</span><span class="n">LEARNT_FEATURE</span><span class="si">}</span><span class="s2">, </span><span class="se">\</span>
<span class="s2">                </span><span class="si">{</span><span class="n">Axis</span><span class="o">.</span><span class="n">LEARNT_FEATURE</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">Axis</span><span class="o">.</span><span class="n">INPUT_OUTPUT_FEATURE</span><span class="si">}</span><span class="s2"> </span><span class="se">\</span>
<span class="s2">                -&gt; </span><span class="si">{</span><span class="n">Axis</span><span class="o">.</span><span class="n">LEARNT_FEATURE</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">Axis</span><span class="o">.</span><span class="n">INPUT_OUTPUT_FEATURE</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Subtracting the parallel component from the gradient leaves only the component that is</span>
        <span class="c1"># orthogonal to the dictionary vectors, i.e. the component that moves around the surface of</span>
        <span class="c1"># the hypersphere.</span>
        <span class="k">return</span> <span class="n">grad</span> <span class="o">-</span> <span class="n">projection</span>

    <span class="k">def</span> <span class="nf">constrain_weights_unit_norm</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Constrain the weights to have unit norm.</span>

<span class="sd">        Note this must be called after each gradient step. This is because optimisers such as Adam</span>
<span class="sd">        don&#39;t strictly follow the gradient, but instead follow a modified gradient that includes</span>
<span class="sd">        momentum. This means that the gradient step can change the norm of the dictionary vectors,</span>
<span class="sd">        even when the hook :meth:`_weight_backward_hook` is applied.</span>

<span class="sd">        Note this can&#39;t be applied directly in the backward hook, as it would interfere with a</span>
<span class="sd">        variety of use cases (e.g. gradient accumulation across mini-batches, concurrency issues</span>
<span class="sd">        with asynchronous operations, etc).</span>

<span class="sd">        Example:</span>
<span class="sd">            &gt;&gt;&gt; import torch</span>
<span class="sd">            &gt;&gt;&gt; layer = UnitNormDecoder(3, 3)</span>
<span class="sd">            &gt;&gt;&gt; layer.weight.data = torch.ones((3, 3)) * 10</span>
<span class="sd">            &gt;&gt;&gt; layer.constrain_weights_unit_norm()</span>
<span class="sd">            &gt;&gt;&gt; row_norms = torch.sum(layer.weight ** 2, dim=1)</span>
<span class="sd">            &gt;&gt;&gt; row_norms.round(decimals=3).tolist()</span>
<span class="sd">            [1.0, 1.0, 1.0]</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_weight</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_weight</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">LearnedActivationBatch</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">InputOutputActivationBatch</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Forward pass.</span>

<span class="sd">        Args:</span>
<span class="sd">            x: Input tensor.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Output of the forward pass.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Prevent the drift of the dictionary vectors away from unit norm. This can happen even</span>
        <span class="c1"># though we remove the gradient information parallel to the dictionary vectors before</span>
        <span class="c1"># applying the gradient step, since optimisers such as Adam don&#39;t strictly follow the</span>
        <span class="c1"># gradient, but instead follow a modified gradient that includes momentum.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">constrain_weights_unit_norm</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_weight</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;String extra representation of the module.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;in_features=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_learnt_features</span><span class="si">}</span><span class="s2">, out_features=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_decoded_features</span><span class="si">}</span><span class="s2">&quot;</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">




<h3 id="sparse_autoencoder.autoencoder.UnitNormDecoder.weight" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">weight</span><span class="p">:</span> <span class="n">DecoderWeights</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>Weight.</p>
  </div>

</div>




<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.autoencoder.UnitNormDecoder.__init__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">learnt_features</span><span class="p">,</span> <span class="n">decoded_features</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">enable_gradient_hook</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Initialize the constrained unit norm linear layer.</p>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>learnt_features</code></b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>Number of learnt features in the autoencoder.</p>
              </div>
            </li>
            <li>
              <b><code>decoded_features</code></b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>Number of decoded (output) features in the autoencoder.</p>
              </div>
            </li>
            <li>
              <b><code>enable_gradient_hook</code></b>
                  (<code>bool</code>, default:
                      <code>True</code>
)
              –
              <div class="doc-md-description">
                <p>Enable the gradient backwards hook (modify the gradient before
applying the gradient step, to maintain unit norm of the dictionary vectors).</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary> <code>sparse_autoencoder/autoencoder/components/unit_norm_decoder.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span>
<span class="normal">85</span>
<span class="normal">86</span>
<span class="normal">87</span>
<span class="normal">88</span>
<span class="normal">89</span>
<span class="normal">90</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">learnt_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">decoded_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">enable_gradient_hook</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Initialize the constrained unit norm linear layer.</span>

<span class="sd">    Args:</span>
<span class="sd">        learnt_features: Number of learnt features in the autoencoder.</span>
<span class="sd">        decoded_features: Number of decoded (output) features in the autoencoder.</span>
<span class="sd">        enable_gradient_hook: Enable the gradient backwards hook (modify the gradient before</span>
<span class="sd">            applying the gradient step, to maintain unit norm of the dictionary vectors).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Create the linear layer as per the standard PyTorch linear layer</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_learnt_features</span> <span class="o">=</span> <span class="n">learnt_features</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_decoded_features</span> <span class="o">=</span> <span class="n">decoded_features</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
            <span class="p">(</span><span class="n">decoded_features</span><span class="p">,</span> <span class="n">learnt_features</span><span class="p">),</span>
        <span class="p">)</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">reset_parameters</span><span class="p">()</span>

    <span class="c1"># Register backward hook to remove any gradient information parallel to the dictionary</span>
    <span class="c1"># vectors (rows of the weight matrix) before applying the gradient step.</span>
    <span class="k">if</span> <span class="n">enable_gradient_hook</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_weight</span><span class="o">.</span><span class="n">register_hook</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_weight_backward_hook</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.autoencoder.UnitNormDecoder.constrain_weights_unit_norm" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">constrain_weights_unit_norm</span><span class="p">()</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Constrain the weights to have unit norm.</p>
<p>Note this must be called after each gradient step. This is because optimisers such as Adam
don't strictly follow the gradient, but instead follow a modified gradient that includes
momentum. This means that the gradient step can change the norm of the dictionary vectors,
even when the hook :meth:<code>_weight_backward_hook</code> is applied.</p>
<p>Note this can't be applied directly in the backward hook, as it would interfere with a
variety of use cases (e.g. gradient accumulation across mini-batches, concurrency issues
with asynchronous operations, etc).</p>

<details class="example" open>
  <summary>Example</summary>
  <blockquote>
<blockquote>
<blockquote>
<p>import torch
layer = UnitNormDecoder(3, 3)
layer.weight.data = torch.ones((3, 3)) * 10
layer.constrain_weights_unit_norm()
row_norms = torch.sum(layer.weight ** 2, dim=1)
row_norms.round(decimals=3).tolist()
[1.0, 1.0, 1.0]</p>
</blockquote>
</blockquote>
</blockquote>
</details>
          <details class="quote">
            <summary> <code>sparse_autoencoder/autoencoder/components/unit_norm_decoder.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">constrain_weights_unit_norm</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Constrain the weights to have unit norm.</span>

<span class="sd">    Note this must be called after each gradient step. This is because optimisers such as Adam</span>
<span class="sd">    don&#39;t strictly follow the gradient, but instead follow a modified gradient that includes</span>
<span class="sd">    momentum. This means that the gradient step can change the norm of the dictionary vectors,</span>
<span class="sd">    even when the hook :meth:`_weight_backward_hook` is applied.</span>

<span class="sd">    Note this can&#39;t be applied directly in the backward hook, as it would interfere with a</span>
<span class="sd">    variety of use cases (e.g. gradient accumulation across mini-batches, concurrency issues</span>
<span class="sd">    with asynchronous operations, etc).</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; layer = UnitNormDecoder(3, 3)</span>
<span class="sd">        &gt;&gt;&gt; layer.weight.data = torch.ones((3, 3)) * 10</span>
<span class="sd">        &gt;&gt;&gt; layer.constrain_weights_unit_norm()</span>
<span class="sd">        &gt;&gt;&gt; row_norms = torch.sum(layer.weight ** 2, dim=1)</span>
<span class="sd">        &gt;&gt;&gt; row_norms.round(decimals=3).tolist()</span>
<span class="sd">        [1.0, 1.0, 1.0]</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_weight</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_weight</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.autoencoder.UnitNormDecoder.extra_repr" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">extra_repr</span><span class="p">()</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>String extra representation of the module.</p>

          <details class="quote">
            <summary> <code>sparse_autoencoder/autoencoder/components/unit_norm_decoder.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;String extra representation of the module.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;in_features=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_learnt_features</span><span class="si">}</span><span class="s2">, out_features=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_decoded_features</span><span class="si">}</span><span class="s2">&quot;</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.autoencoder.UnitNormDecoder.forward" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Forward pass.</p>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>x</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.LearnedActivationBatch" href="../tensor_types/#sparse_autoencoder.tensor_types.LearnedActivationBatch">LearnedActivationBatch</a></code>)
              –
              <div class="doc-md-description">
                <p>Input tensor.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
    <th class="field-name">Returns:</th>
    <td class="field-body">
      <ul class="first simple">
          <li>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.InputOutputActivationBatch" href="../tensor_types/#sparse_autoencoder.tensor_types.InputOutputActivationBatch">InputOutputActivationBatch</a></code>
            –
            <div class="doc-md-description">
              <p>Output of the forward pass.</p>
            </div>
          </li>
      </ul>
    </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary> <code>sparse_autoencoder/autoencoder/components/unit_norm_decoder.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">LearnedActivationBatch</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">InputOutputActivationBatch</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Forward pass.</span>

<span class="sd">    Args:</span>
<span class="sd">        x: Input tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Output of the forward pass.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Prevent the drift of the dictionary vectors away from unit norm. This can happen even</span>
    <span class="c1"># though we remove the gradient information parallel to the dictionary vectors before</span>
    <span class="c1"># applying the gradient step, since optimisers such as Adam don&#39;t strictly follow the</span>
    <span class="c1"># gradient, but instead follow a modified gradient that includes momentum.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">constrain_weights_unit_norm</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_weight</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.autoencoder.UnitNormDecoder.reset_parameters" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">reset_parameters</span><span class="p">()</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Initialize or reset the parameters.</p>

<details class="example" open>
  <summary>Example</summary>
  <blockquote>
<blockquote>
<blockquote>
<p>import torch</p>
<h4 id="sparse_autoencoder.autoencoder.UnitNormDecoder.reset_parameters--create-a-layer-with-4-columns-learnt-features-and-3-rows-decoded-features">Create a layer with 4 columns (learnt features) and 3 rows (decoded features)</h4>
<p>layer = UnitNormDecoder(learnt_features=4, decoded_features=3)
layer.reset_parameters()</p>
<h4 id="sparse_autoencoder.autoencoder.UnitNormDecoder.reset_parameters--get-the-norm-across-the-rows-by-summing-across-the-columns">Get the norm across the rows (by summing across the columns)</h4>
<p>row_norms = torch.sum(layer.weight ** 2, dim=1)
row_norms.round(decimals=3).tolist()
[1.0, 1.0, 1.0]</p>
</blockquote>
</blockquote>
</blockquote>
</details>
          <details class="quote">
            <summary> <code>sparse_autoencoder/autoencoder/components/unit_norm_decoder.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Initialize or reset the parameters.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; # Create a layer with 4 columns (learnt features) and 3 rows (decoded features)</span>
<span class="sd">        &gt;&gt;&gt; layer = UnitNormDecoder(learnt_features=4, decoded_features=3)</span>
<span class="sd">        &gt;&gt;&gt; layer.reset_parameters()</span>
<span class="sd">        &gt;&gt;&gt; # Get the norm across the rows (by summing across the columns)</span>
<span class="sd">        &gt;&gt;&gt; row_norms = torch.sum(layer.weight ** 2, dim=1)</span>
<span class="sd">        &gt;&gt;&gt; row_norms.round(decimals=3).tolist()</span>
<span class="sd">        [1.0, 1.0, 1.0]</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Initialize the weights with a normal distribution. Note we don&#39;t use e.g. kaiming</span>
    <span class="c1"># normalisation here, since we immediately scale the weights to have unit norm (so the</span>
    <span class="c1"># initial standard deviation doesn&#39;t matter). Note also that `init.normal_` is in place.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_weight</span><span class="p">:</span> <span class="n">EncoderWeights</span> <span class="o">=</span> <span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_weight</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Scale so that each row has unit norm</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_weight</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_weight</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>




  </div>

  </div>

</div>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../activation_store/utils/extend_resize/" class="btn btn-neutral float-left" title="extend_resize"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="abstract_autoencoder/" class="btn btn-neutral float-right" title="abstract_autoencoder">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../activation_store/utils/extend_resize/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="abstract_autoencoder/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "../..";</script>
    <script src="../../js/theme_extra.js"></script>
    <script src="../../js/theme.js"></script>
      <script src="../../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
