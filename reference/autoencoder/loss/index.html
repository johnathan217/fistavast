<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../../../img/favicon.ico" />
    <title>loss - Sparse Autoencoder</title>
    <link rel="stylesheet" href="../../../css/theme.css" />
    <link rel="stylesheet" href="../../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="../../../assets/_mkdocstrings.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "loss";
        var mkdocs_page_input_path = "reference/autoencoder/loss.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../../.." class="icon icon-home"> Sparse Autoencoder
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../../..">Home</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../../../getting_started/">Getting Started</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../../../contributing/">Contributing</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../../../citation/">Citation</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Reference</span></p>
              <ul class="current">
                  <li class="toctree-l1"><a class="reference internal" href="../../">Home</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">activation_resampler</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../activation_resampler/">Index</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../activation_resampler/abstract_activation_resampler/">abstract_activation_resampler</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../activation_resampler/activation_resampler/">activation_resampler</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">activation_store</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../activation_store/">Index</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../activation_store/base_store/">base_store</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../activation_store/disk_store/">disk_store</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../activation_store/list_store/">list_store</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../activation_store/tensor_store/">tensor_store</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="#">utils</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../../activation_store/utils/">Index</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../activation_store/utils/extend_resize/">extend_resize</a>
                </li>
    </ul>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1 current"><a class="reference internal current" href="#">autoencoder</a>
    <ul class="current">
                <li class="toctree-l2"><a class="reference internal" href="../">Index</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../abstract_autoencoder/">abstract_autoencoder</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="#">components</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../components/">Index</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../components/abstract_decoder/">abstract_decoder</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../components/abstract_encoder/">abstract_encoder</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../components/abstract_outer_bias/">abstract_outer_bias</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../components/linear_encoder/">linear_encoder</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../components/tied_bias/">tied_bias</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../components/unit_norm_decoder/">unit_norm_decoder</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../components/unit_norm_linear/">unit_norm_linear</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../fista_autoencoder/">fista_autoencoder</a>
                </li>
                <li class="toctree-l2 current"><a class="reference internal current" href="./">loss</a>
    <ul class="current">
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../model/">model</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">loss</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../loss/">Index</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../loss/abstract_loss/">abstract_loss</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../loss/fista/">fista</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../loss/learned_activations_l1/">learned_activations_l1</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../loss/mse_reconstruction_loss/">mse_reconstruction_loss</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../loss/reducer/">reducer</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">metrics</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../metrics/">Index</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../metrics/abstract_metric/">abstract_metric</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="#">generate</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../../metrics/generate/">Index</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../metrics/generate/abstract_generate_metric/">abstract_generate_metric</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="#">train</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../../metrics/train/">Index</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../metrics/train/abstract_train_metric/">abstract_train_metric</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="#">validate</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../../metrics/validate/">Index</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../metrics/validate/abstract_validate_metric/">abstract_validate_metric</a>
                </li>
    </ul>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">optimizer</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../optimizer/">Index</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../optimizer/abstract_optimizer/">abstract_optimizer</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../optimizer/adam_with_reset/">adam_with_reset</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">source_data</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../source_data/">Index</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../source_data/abstract_dataset/">abstract_dataset</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../source_data/c4_pre_tokenized/">c4_pre_tokenized</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../source_data/pile_uncopyrighted/">pile_uncopyrighted</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../source_data/pretokenized_dataset/">pretokenized_dataset</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../source_data/random_int/">random_int</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../source_data/text_dataset/">text_dataset</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">source_model</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../source_model/">Index</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../source_model/store_activations_hook/">store_activations_hook</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">src_model</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../src_model/">Index</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../src_model/store_activations_hook/">store_activations_hook</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../tensor_types/">tensor_types</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">train</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../train/">Index</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../train/abstract_pipeline/">abstract_pipeline</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../train/generate_activations/">generate_activations</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="#">metrics</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../../train/metrics/">Index</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../train/metrics/capacity/">capacity</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../train/metrics/feature_density/">feature_density</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../train/pipeline/">pipeline</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../train/resample_neurons/">resample_neurons</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../train/sweep_config/">sweep_config</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../train/train_autoencoder/">train_autoencoder</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="#">utils</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../../train/utils/">Index</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../train/utils/get_model_device/">get_model_device</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../train/utils/wandb_sweep_types/">wandb_sweep_types</a>
                </li>
    </ul>
                </li>
    </ul>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../..">Sparse Autoencoder</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../.." class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">Reference</li>
          <li class="breadcrumb-item">autoencoder</li>
      <li class="breadcrumb-item active">loss</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <div class="doc doc-object doc-module">



<a id="sparse_autoencoder.autoencoder.loss"></a>
  <div class="doc doc-contents first">
  
      <p>Loss function for the Sparse Autoencoder.</p>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">




<h2 id="sparse_autoencoder.autoencoder.loss.l1_loss" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">l1_loss</span><span class="p">(</span><span class="n">learned_activations</span><span class="p">)</span></code>

</h2>


  <div class="doc doc-contents ">
  
      <p>L1 Loss on Learned Activations.</p>
<p>L1 loss penalty is the absolute sum of the learned activations. The L1 penalty is this
multiplied by the l1_coefficient (designed to encourage sparsity).</p>
<p>Examples:</p>
<blockquote>
<blockquote>
<blockquote>
<p>learned_activations = torch.tensor([[2.0, -3], [2.0, -3]])
l1_loss(learned_activations)
tensor([5., 5.])</p>
</blockquote>
</blockquote>
</blockquote>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>learned_activations</code></b>
                  (<code><span title="jaxtyping.Float">Float</span>[<span title="torch.Tensor">Tensor</span>, &#39;item learned_features&#39;]</code>)
              –
              <div class="doc-md-description">
                <p>Activations from the hidden layer.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
    <th class="field-name">Returns:</th>
    <td class="field-body">
      <ul class="first simple">
          <li>
                <code><span title="jaxtyping.Float">Float</span>[<span title="torch.Tensor">Tensor</span>, &#39; item&#39;]</code>
            –
            <div class="doc-md-description">
              <p>L1 loss on learned activations, summed over the features dimension.</p>
            </div>
          </li>
      </ul>
    </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary> <code>sparse_autoencoder/autoencoder/loss.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">l1_loss</span><span class="p">(</span><span class="n">learned_activations</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="s2">&quot;item learned_features&quot;</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="s2">&quot; item&quot;</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;L1 Loss on Learned Activations.</span>

<span class="sd">    L1 loss penalty is the absolute sum of the learned activations. The L1 penalty is this</span>
<span class="sd">    multiplied by the l1_coefficient (designed to encourage sparsity).</span>

<span class="sd">    Examples:</span>
<span class="sd">    &gt;&gt;&gt; learned_activations = torch.tensor([[2.0, -3], [2.0, -3]])</span>
<span class="sd">    &gt;&gt;&gt; l1_loss(learned_activations)</span>
<span class="sd">    tensor([5., 5.])</span>

<span class="sd">    Args:</span>
<span class="sd">        learned_activations: Activations from the hidden layer.</span>

<span class="sd">    Returns:</span>
<span class="sd">        L1 loss on learned activations, summed over the features dimension.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">learned_activations</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h2 id="sparse_autoencoder.autoencoder.loss.reconstruction_loss" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">reconstruction_loss</span><span class="p">(</span><span class="n">input_activations</span><span class="p">,</span> <span class="n">output_activations</span><span class="p">)</span></code>

</h2>


  <div class="doc doc-contents ">
  
      <p>Reconstruction Loss (MSE).</p>
<p>MSE reconstruction loss is calculated as the mean squared error between each each input vector
and it's corresponding decoded vector. The original paper found that models trained with some
loss functions such as cross-entropy loss generally prefer to represent features
polysemantically, whereas models trained with MSE may achieve the same loss for both
polysemantic and monosemantic representations of true features.</p>
<p>Examples:</p>
<blockquote>
<blockquote>
<blockquote>
<p>input_activations = torch.tensor([[5.0, 4], [3.0, 4]])
output_activations = torch.tensor([[1.0, 5], [1.0, 5]])
reconstruction_loss(input_activations, output_activations)
tensor([8.5000, 2.5000])</p>
</blockquote>
</blockquote>
</blockquote>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>input_activations</code></b>
                  (<code><span title="jaxtyping.Float">Float</span>[<span title="torch.Tensor">Tensor</span>, &#39;item input_features&#39;]</code>)
              –
              <div class="doc-md-description">
                <p>Input activations.</p>
              </div>
            </li>
            <li>
              <b><code>output_activations</code></b>
                  (<code><span title="jaxtyping.Float">Float</span>[<span title="torch.Tensor">Tensor</span>, &#39;item input_features&#39;]</code>)
              –
              <div class="doc-md-description">
                <p>Reconstructed activations.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
    <th class="field-name">Returns:</th>
    <td class="field-body">
      <ul class="first simple">
          <li>
                <code><span title="jaxtyping.Float">Float</span>[<span title="torch.Tensor">Tensor</span>, &#39; item&#39;]</code>
            –
            <div class="doc-md-description">
              <p>Mean Squared Error reconstruction loss, over the features dimension.</p>
            </div>
          </li>
      </ul>
    </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary> <code>sparse_autoencoder/autoencoder/loss.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">reconstruction_loss</span><span class="p">(</span>
    <span class="n">input_activations</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="s2">&quot;item input_features&quot;</span><span class="p">],</span>
    <span class="n">output_activations</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="s2">&quot;item input_features&quot;</span><span class="p">],</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="s2">&quot; item&quot;</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Reconstruction Loss (MSE).</span>

<span class="sd">    MSE reconstruction loss is calculated as the mean squared error between each each input vector</span>
<span class="sd">    and it&#39;s corresponding decoded vector. The original paper found that models trained with some</span>
<span class="sd">    loss functions such as cross-entropy loss generally prefer to represent features</span>
<span class="sd">    polysemantically, whereas models trained with MSE may achieve the same loss for both</span>
<span class="sd">    polysemantic and monosemantic representations of true features.</span>

<span class="sd">    Examples:</span>
<span class="sd">    &gt;&gt;&gt; input_activations = torch.tensor([[5.0, 4], [3.0, 4]])</span>
<span class="sd">    &gt;&gt;&gt; output_activations = torch.tensor([[1.0, 5], [1.0, 5]])</span>
<span class="sd">    &gt;&gt;&gt; reconstruction_loss(input_activations, output_activations)</span>
<span class="sd">    tensor([8.5000, 2.5000])</span>

<span class="sd">    Args:</span>
<span class="sd">        input_activations: Input activations.</span>
<span class="sd">        output_activations: Reconstructed activations.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Mean Squared Error reconstruction loss, over the features dimension.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">mse_loss</span><span class="p">(</span><span class="n">input_activations</span><span class="p">,</span> <span class="n">output_activations</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h2 id="sparse_autoencoder.autoencoder.loss.sae_training_loss" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">sae_training_loss</span><span class="p">(</span><span class="n">reconstruction_loss_mse</span><span class="p">,</span> <span class="n">l1_loss_learned_activations</span><span class="p">,</span> <span class="n">l1_coefficient</span><span class="p">)</span></code>

</h2>


  <div class="doc doc-contents ">
  
      <p>Loss Function for the Sparse Autoencoder.</p>
<p>The original paper used L2 reconstruction loss, plus l1 loss on the hidden (learned)
activations.</p>
<p>https://transformer-circuits.pub/2023/monosemantic-features/index.html#setup-autoencoder-motivation</p>

<details class="warning" open>
  <summary>Warning</summary>
  <p>It isn't meaningful to compare training loss across hyperparameters that change the loss
function, such as L1 coefficients.</p>
</details>


<p><strong></strong></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">reconstruction_loss_mse</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">8.5000</span><span class="p">,</span> <span class="mf">2.5000</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">l1_loss_learned_activations</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">l1_coefficient</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sae_training_loss</span><span class="p">(</span><span class="n">reconstruction_loss_mse</span><span class="p">,</span> <span class="n">l1_loss_learned_activations</span><span class="p">,</span> <span class="n">l1_coefficient</span><span class="p">)</span>
<span class="go">tensor([9., 3.])</span>
</code></pre></div>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>reconstruction_loss_mse</code></b>
                  (<code><span title="jaxtyping.Float">Float</span>[<span title="torch.Tensor">Tensor</span>, &#39; item&#39;]</code>)
              –
              <div class="doc-md-description">
                <p>MSE reconstruction loss.</p>
              </div>
            </li>
            <li>
              <b><code>l1_loss_learned_activations</code></b>
                  (<code><span title="jaxtyping.Float">Float</span>[<span title="torch.Tensor">Tensor</span>, &#39; item&#39;]</code>)
              –
              <div class="doc-md-description">
                <p>L1 loss on learned activations.</p>
              </div>
            </li>
            <li>
              <b><code>l1_coefficient</code></b>
                  (<code>float</code>)
              –
              <div class="doc-md-description">
                <p>L1 coefficient. The original paper experimented with L1 coefficients of
[0.01, 0.008, 0.006, 0.004, 0.001]. They used 250 tokens per prompt, so as an
approximate guide if you use e.g. 2x this number of tokens you might consider using 0.5x
the l1 coefficient.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
    <th class="field-name">Returns:</th>
    <td class="field-body">
      <ul class="first simple">
          <li>
                <code><span title="jaxtyping.Float">Float</span>[<span title="torch.Tensor">Tensor</span>, &#39; item&#39;]</code>
            –
            <div class="doc-md-description">
              <p>Overall training loss.</p>
            </div>
          </li>
      </ul>
    </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary> <code>sparse_autoencoder/autoencoder/loss.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span>
<span class="normal">85</span>
<span class="normal">86</span>
<span class="normal">87</span>
<span class="normal">88</span>
<span class="normal">89</span>
<span class="normal">90</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">sae_training_loss</span><span class="p">(</span>
    <span class="n">reconstruction_loss_mse</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="s2">&quot; item&quot;</span><span class="p">],</span>
    <span class="n">l1_loss_learned_activations</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="s2">&quot; item&quot;</span><span class="p">],</span>
    <span class="n">l1_coefficient</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="s2">&quot; item&quot;</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Loss Function for the Sparse Autoencoder.</span>

<span class="sd">    The original paper used L2 reconstruction loss, plus l1 loss on the hidden (learned)</span>
<span class="sd">    activations.</span>

<span class="sd">    https://transformer-circuits.pub/2023/monosemantic-features/index.html#setup-autoencoder-motivation</span>

<span class="sd">    Warning:</span>
<span class="sd">        It isn&#39;t meaningful to compare training loss across hyperparameters that change the loss</span>
<span class="sd">        function, such as L1 coefficients.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; reconstruction_loss_mse = torch.tensor([8.5000, 2.5000])</span>
<span class="sd">        &gt;&gt;&gt; l1_loss_learned_activations = torch.tensor([1., 1.])</span>
<span class="sd">        &gt;&gt;&gt; l1_coefficient = 0.5</span>
<span class="sd">        &gt;&gt;&gt; sae_training_loss(reconstruction_loss_mse, l1_loss_learned_activations, l1_coefficient)</span>
<span class="sd">        tensor([9., 3.])</span>

<span class="sd">    Args:</span>
<span class="sd">        reconstruction_loss_mse: MSE reconstruction loss.</span>
<span class="sd">        l1_loss_learned_activations: L1 loss on learned activations.</span>
<span class="sd">        l1_coefficient: L1 coefficient. The original paper experimented with L1 coefficients of</span>
<span class="sd">            [0.01, 0.008, 0.006, 0.004, 0.001]. They used 250 tokens per prompt, so as an</span>
<span class="sd">            approximate guide if you use e.g. 2x this number of tokens you might consider using 0.5x</span>
<span class="sd">            the l1 coefficient.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Overall training loss.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">reconstruction_loss_mse</span> <span class="o">+</span> <span class="n">l1_loss_learned_activations</span> <span class="o">*</span> <span class="n">l1_coefficient</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../fista_autoencoder/" class="btn btn-neutral float-left" title="fista_autoencoder"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../model/" class="btn btn-neutral float-right" title="model">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../fista_autoencoder/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../model/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../../../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "../../..";</script>
    <script src="../../../js/theme_extra.js"></script>
    <script src="../../../js/theme.js"></script>
      <script src="../../../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
