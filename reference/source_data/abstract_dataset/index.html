<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../../../img/favicon.ico" />
    <title>abstract_dataset - Sparse Autoencoder</title>
    <link rel="stylesheet" href="../../../css/theme.css" />
    <link rel="stylesheet" href="../../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="../../../assets/_mkdocstrings.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "abstract_dataset";
        var mkdocs_page_input_path = "reference/source_data/abstract_dataset.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../../.." class="icon icon-home"> Sparse Autoencoder
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../../..">Home</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../../../getting_started/">Getting Started</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../../../contributing/">Contributing</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../../../citation/">Citation</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Reference</span></p>
              <ul class="current">
                  <li class="toctree-l1"><a class="reference internal" href="../../">Home</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">activation_resampler</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../activation_resampler/">Index</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../activation_resampler/abstract_activation_resampler/">abstract_activation_resampler</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../activation_resampler/activation_resampler/">activation_resampler</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">activation_store</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../activation_store/">Index</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../activation_store/base_store/">base_store</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../activation_store/disk_store/">disk_store</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../activation_store/list_store/">list_store</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../activation_store/tensor_store/">tensor_store</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="#">utils</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../../activation_store/utils/">Index</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../activation_store/utils/extend_resize/">extend_resize</a>
                </li>
    </ul>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">autoencoder</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../autoencoder/">Index</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../autoencoder/abstract_autoencoder/">abstract_autoencoder</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="#">components</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../../autoencoder/components/">Index</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../autoencoder/components/abstract_decoder/">abstract_decoder</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../autoencoder/components/abstract_encoder/">abstract_encoder</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../autoencoder/components/abstract_outer_bias/">abstract_outer_bias</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../autoencoder/components/linear_encoder/">linear_encoder</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../autoencoder/components/tied_bias/">tied_bias</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../autoencoder/components/unit_norm_decoder/">unit_norm_decoder</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../autoencoder/components/unit_norm_linear/">unit_norm_linear</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../autoencoder/fista_autoencoder/">fista_autoencoder</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../autoencoder/loss/">loss</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../autoencoder/model/">model</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">loss</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../loss/">Index</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../loss/abstract_loss/">abstract_loss</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../loss/fista/">fista</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../loss/learned_activations_l1/">learned_activations_l1</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../loss/mse_reconstruction_loss/">mse_reconstruction_loss</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../loss/reducer/">reducer</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">metrics</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../metrics/">Index</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../metrics/abstract_metric/">abstract_metric</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="#">generate</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../../metrics/generate/">Index</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../metrics/generate/abstract_generate_metric/">abstract_generate_metric</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="#">train</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../../metrics/train/">Index</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../metrics/train/abstract_train_metric/">abstract_train_metric</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="#">validate</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../../metrics/validate/">Index</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../metrics/validate/abstract_validate_metric/">abstract_validate_metric</a>
                </li>
    </ul>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">optimizer</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../optimizer/">Index</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../optimizer/abstract_optimizer/">abstract_optimizer</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../optimizer/adam_with_reset/">adam_with_reset</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1 current"><a class="reference internal current" href="#">source_data</a>
    <ul class="current">
                <li class="toctree-l2"><a class="reference internal" href="../">Index</a>
                </li>
                <li class="toctree-l2 current"><a class="reference internal current" href="./">abstract_dataset</a>
    <ul class="current">
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../c4_pre_tokenized/">c4_pre_tokenized</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../pile_uncopyrighted/">pile_uncopyrighted</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../pretokenized_dataset/">pretokenized_dataset</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../random_int/">random_int</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../text_dataset/">text_dataset</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">source_model</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../source_model/">Index</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../source_model/store_activations_hook/">store_activations_hook</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">src_model</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../src_model/">Index</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../src_model/store_activations_hook/">store_activations_hook</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../tensor_types/">tensor_types</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">train</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../train/">Index</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../train/abstract_pipeline/">abstract_pipeline</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../train/generate_activations/">generate_activations</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="#">metrics</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../../train/metrics/">Index</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../train/metrics/capacity/">capacity</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../train/metrics/feature_density/">feature_density</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../train/pipeline/">pipeline</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../train/resample_neurons/">resample_neurons</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../train/sweep_config/">sweep_config</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../train/train_autoencoder/">train_autoencoder</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="#">utils</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../../train/utils/">Index</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../train/utils/get_model_device/">get_model_device</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../train/utils/wandb_sweep_types/">wandb_sweep_types</a>
                </li>
    </ul>
                </li>
    </ul>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../..">Sparse Autoencoder</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../.." class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">Reference</li>
          <li class="breadcrumb-item">source_data</li>
      <li class="breadcrumb-item active">abstract_dataset</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <div class="doc doc-object doc-module">



<a id="sparse_autoencoder.source_data.abstract_dataset"></a>
  <div class="doc doc-contents first">
  
      <p>Abstract tokenized prompts dataset class.</p>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">




<h2 id="sparse_autoencoder.source_data.abstract_dataset.HuggingFaceDatasetItem" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">HuggingFaceDatasetItem</span> <span class="o">=</span> <span class="n">TypeVar</span><span class="p">(</span><span class="s1">&#39;HuggingFaceDatasetItem&#39;</span><span class="p">,</span> <span class="n">bound</span><span class="o">=</span><span class="n">Any</span><span class="p">)</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-module-attribute"><code>module-attribute</code></small>
  </span>

</h2>


  <div class="doc doc-contents ">
  
      <p>Hugging face dataset item typed dict.</p>
<p>When extending :class:<code>SourceDataset</code> you should create a <code>TypedDict</code> that matches the structure of
each dataset item in the underlying Hugging Face dataset.</p>

<details class="example" open>
  <summary>Example</summary>
  <p>With the <a href="https://huggingface.co/datasets/monology/pile-uncopyrighted">Uncopyrighted
Pile</a> this should be a typed dict
with text and meta properties.</p>
<blockquote>
<blockquote>
<blockquote>
<p>class PileUncopyrightedSourceDataBatch(TypedDict):
...    text: list[str]
...    meta: list[dict[str, dict[str, str]]]</p>
</blockquote>
</blockquote>
</blockquote>
</details>  </div>

</div>

<div class="doc doc-object doc-attribute">




<h2 id="sparse_autoencoder.source_data.abstract_dataset.TokenizedPrompt" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">TokenizedPrompt</span> <span class="o">=</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-module-attribute"><code>module-attribute</code></small>
  </span>

</h2>


  <div class="doc doc-contents ">
  
      <p>A tokenized prompt.</p>
  </div>

</div>


<div class="doc doc-object doc-class">




<h2 id="sparse_autoencoder.source_data.abstract_dataset.SourceDataset" class="doc doc-heading">
          <code>SourceDataset</code>


</h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><span title="abc.ABC">ABC</span></code>, <code><span title="typing.Generic">Generic</span>[<a class="autorefs autorefs-internal" title="sparse_autoencoder.source_data.abstract_dataset.HuggingFaceDatasetItem" href="#sparse_autoencoder.source_data.abstract_dataset.HuggingFaceDatasetItem">HuggingFaceDatasetItem</a>]</code></p>

  
      <p>Abstract source dataset.</p>
<p>Source dataset that is used to generate the activations dataset (by running forward passes of
the source model with this data). It should contain prompts that have been tokenized with no
padding tokens (apart from an optional single first padding token). This enables efficient
generation of the activations dataset.</p>
<p>Wraps an HuggingFace IterableDataset.</p>

            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/source_data/abstract_dataset.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SourceDataset</span><span class="p">(</span><span class="n">ABC</span><span class="p">,</span> <span class="n">Generic</span><span class="p">[</span><span class="n">HuggingFaceDatasetItem</span><span class="p">]):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Abstract source dataset.</span>

<span class="sd">    Source dataset that is used to generate the activations dataset (by running forward passes of</span>
<span class="sd">    the source model with this data). It should contain prompts that have been tokenized with no</span>
<span class="sd">    padding tokens (apart from an optional single first padding token). This enables efficient</span>
<span class="sd">    generation of the activations dataset.</span>

<span class="sd">    Wraps an HuggingFace IterableDataset.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">context_size</span><span class="p">:</span> <span class="nb">int</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Number of tokens in the context window.</span>

<span class="sd">    The paper *Towards Monosemanticity: Decomposing Language Models With Dictionary Learning* used</span>
<span class="sd">    a context size of 250.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">dataset</span><span class="p">:</span> <span class="n">IterableDataset</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Underlying HuggingFace IterableDataset.</span>

<span class="sd">    Warning:</span>
<span class="sd">        Hugging Face `Dataset` objects are confusingly not the same as PyTorch `Dataset` objects.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">preprocess</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">source_batch</span><span class="p">:</span> <span class="n">HuggingFaceDatasetItem</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">context_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TokenizedPrompts</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Preprocess function.</span>

<span class="sd">        Takes a `preprocess_batch_size` ($m$) batch of source data (which may e.g. include string</span>
<span class="sd">        prompts), and returns a dict with a single key of `input_ids` and a value of an arbitrary</span>
<span class="sd">        length list ($n$) of tokenized prompts. Note that $m$ does not have to be equal to $n$.</span>

<span class="sd">        Applied to the dataset with the [Hugging Face</span>
<span class="sd">        Dataset](https://huggingface.co/docs/datasets/v2.14.5/en/package_reference/main_classes#datasets.Dataset.map)</span>
<span class="sd">        `map` function.</span>

<span class="sd">        Warning:</span>
<span class="sd">            The returned tokenized prompts should not have any padding tokens (apart from an</span>
<span class="sd">            optional single first padding token).</span>

<span class="sd">        Args:</span>
<span class="sd">            source_batch: A batch of source data. For example, with The Pile dataset this would be a</span>
<span class="sd">                dict including the key &quot;text&quot; with a value of a list of strings (not yet tokenized).</span>
<span class="sd">            context_size: The context size to use when returning a list of tokenized prompts.</span>
<span class="sd">                *Towards Monosemanticity: Decomposing Language Models With Dictionary Learning* used</span>
<span class="sd">                a context size of 250.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">dataset_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">dataset_split</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">context_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">buffer_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
        <span class="n">preprocess_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialise the dataset.</span>

<span class="sd">        Loads the dataset with streaming from HuggingFace, dds preprocessing and shuffling to the</span>
<span class="sd">        underlying Hugging Face `IterableDataset`.</span>

<span class="sd">        Args:</span>
<span class="sd">            dataset_path: The path to the dataset on Hugging Face.</span>
<span class="sd">            dataset_split: Dataset split (e.g. `train`).</span>
<span class="sd">            context_size: The context size to use when returning a list of tokenized prompts.</span>
<span class="sd">                *Towards Monosemanticity: Decomposing Language Models With Dictionary Learning* used</span>
<span class="sd">                a context size of 250.</span>
<span class="sd">            buffer_size: The buffer size to use when shuffling the dataset. As the dataset is</span>
<span class="sd">                streamed, this just pre-downloads at least `buffer_size` items and then shuffles</span>
<span class="sd">                just that buffer. Note that the generated activations should also be shuffled before</span>
<span class="sd">                training the sparse autoencoder, so a large buffer may not be strictly necessary</span>
<span class="sd">                here. Note also that this is the number of items in the dataset (e.g. number of</span>
<span class="sd">                prompts) and is typically significantly less than the number of tokenized prompts</span>
<span class="sd">                once the preprocessing function has been applied.</span>
<span class="sd">            preprocess_batch_size: The batch size to use just for preprocessing the dataset (e.g.</span>
<span class="sd">                tokenizing prompts).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">context_size</span> <span class="o">=</span> <span class="n">context_size</span>

        <span class="c1"># Load the dataset</span>
        <span class="n">dataset</span><span class="p">:</span> <span class="n">IterableDataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="n">dataset_path</span><span class="p">,</span> <span class="n">streaming</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="n">dataset_split</span><span class="p">)</span>  <span class="c1"># type: ignore</span>

        <span class="c1"># Setup preprocessing</span>
        <span class="n">existing_columns</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">dataset</span><span class="p">))</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
        <span class="n">mapped_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">preprocess</span><span class="p">,</span>
            <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="n">preprocess_batch_size</span><span class="p">,</span>
            <span class="n">fn_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;context_size&quot;</span><span class="p">:</span> <span class="n">context_size</span><span class="p">},</span>
            <span class="n">remove_columns</span><span class="o">=</span><span class="n">existing_columns</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Setup approximate shuffling. As the dataset is streamed, this just pre-downloads at least</span>
        <span class="c1"># `buffer_size` items and then shuffles just that buffer.</span>
        <span class="c1"># https://huggingface.co/docs/datasets/v2.14.5/stream#shuffle</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span> <span class="o">=</span> <span class="n">mapped_dataset</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">buffer_size</span><span class="o">=</span><span class="n">buffer_size</span><span class="p">)</span>

    <span class="nd">@final</span>
    <span class="k">def</span> <span class="fm">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>  <span class="c1"># noqa: ANN401</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Iterate Dunder Method.</span>

<span class="sd">        Enables direct access to :attr:`dataset` with e.g. `for` loops.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="fm">__iter__</span><span class="p">()</span>

    <span class="nd">@final</span>
    <span class="k">def</span> <span class="fm">__next__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>  <span class="c1"># noqa: ANN401</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Next Dunder Method.</span>

<span class="sd">        Enables direct access to :attr:`dataset` with e.g. `next` calls.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="bp">self</span><span class="p">))</span>

    <span class="nd">@final</span>
    <span class="k">def</span> <span class="nf">get_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">DataLoader</span><span class="p">[</span><span class="n">TorchTokenizedPrompts</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get a PyTorch DataLoader.</span>

<span class="sd">        Args:</span>
<span class="sd">            batch_size: The batch size to use.</span>

<span class="sd">        Returns:</span>
<span class="sd">            PyTorch DataLoader.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">torch_dataset</span><span class="p">:</span> <span class="n">TorchDataset</span><span class="p">[</span><span class="n">TorchTokenizedPrompts</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">with_format</span><span class="p">(</span><span class="s2">&quot;torch&quot;</span><span class="p">)</span>  <span class="c1"># type: ignore</span>

        <span class="k">return</span> <span class="n">DataLoader</span><span class="p">[</span><span class="n">TorchTokenizedPrompts</span><span class="p">](</span>
            <span class="n">torch_dataset</span><span class="p">,</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
            <span class="c1"># Shuffle is most efficiently done with the `shuffle` method on the dataset itself, not</span>
            <span class="c1"># here.</span>
            <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">




<h3 id="sparse_autoencoder.source_data.abstract_dataset.SourceDataset.context_size" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">context_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">context_size</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>Number of tokens in the context window.</p>
<p>The paper <em>Towards Monosemanticity: Decomposing Language Models With Dictionary Learning</em> used
a context size of 250.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">




<h3 id="sparse_autoencoder.source_data.abstract_dataset.SourceDataset.dataset" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">dataset</span><span class="p">:</span> <span class="n">IterableDataset</span> <span class="o">=</span> <span class="n">mapped_dataset</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">buffer_size</span><span class="o">=</span><span class="n">buffer_size</span><span class="p">)</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>Underlying HuggingFace IterableDataset.</p>

<details class="warning" open>
  <summary>Warning</summary>
  <p>Hugging Face <code>Dataset</code> objects are confusingly not the same as PyTorch <code>Dataset</code> objects.</p>
</details>  </div>

</div>




<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.source_data.abstract_dataset.SourceDataset.__init__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">dataset_path</span><span class="p">,</span> <span class="n">dataset_split</span><span class="p">,</span> <span class="n">context_size</span><span class="p">,</span> <span class="n">buffer_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">preprocess_batch_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-abstractmethod"><code>abstractmethod</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>Initialise the dataset.</p>
<p>Loads the dataset with streaming from HuggingFace, dds preprocessing and shuffling to the
underlying Hugging Face <code>IterableDataset</code>.</p>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>dataset_path</code></b>
                  (<code>str</code>)
              –
              <div class="doc-md-description">
                <p>The path to the dataset on Hugging Face.</p>
              </div>
            </li>
            <li>
              <b><code>dataset_split</code></b>
                  (<code>str</code>)
              –
              <div class="doc-md-description">
                <p>Dataset split (e.g. <code>train</code>).</p>
              </div>
            </li>
            <li>
              <b><code>context_size</code></b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>The context size to use when returning a list of tokenized prompts.
<em>Towards Monosemanticity: Decomposing Language Models With Dictionary Learning</em> used
a context size of 250.</p>
              </div>
            </li>
            <li>
              <b><code>buffer_size</code></b>
                  (<code>int</code>, default:
                      <code>1000</code>
)
              –
              <div class="doc-md-description">
                <p>The buffer size to use when shuffling the dataset. As the dataset is
streamed, this just pre-downloads at least <code>buffer_size</code> items and then shuffles
just that buffer. Note that the generated activations should also be shuffled before
training the sparse autoencoder, so a large buffer may not be strictly necessary
here. Note also that this is the number of items in the dataset (e.g. number of
prompts) and is typically significantly less than the number of tokenized prompts
once the preprocessing function has been applied.</p>
              </div>
            </li>
            <li>
              <b><code>preprocess_batch_size</code></b>
                  (<code>int</code>, default:
                      <code>1000</code>
)
              –
              <div class="doc-md-description">
                <p>The batch size to use just for preprocessing the dataset (e.g.
tokenizing prompts).</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary> <code>sparse_autoencoder/source_data/abstract_dataset.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@abstractmethod</span>
<span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">dataset_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">dataset_split</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">context_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">buffer_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
    <span class="n">preprocess_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Initialise the dataset.</span>

<span class="sd">    Loads the dataset with streaming from HuggingFace, dds preprocessing and shuffling to the</span>
<span class="sd">    underlying Hugging Face `IterableDataset`.</span>

<span class="sd">    Args:</span>
<span class="sd">        dataset_path: The path to the dataset on Hugging Face.</span>
<span class="sd">        dataset_split: Dataset split (e.g. `train`).</span>
<span class="sd">        context_size: The context size to use when returning a list of tokenized prompts.</span>
<span class="sd">            *Towards Monosemanticity: Decomposing Language Models With Dictionary Learning* used</span>
<span class="sd">            a context size of 250.</span>
<span class="sd">        buffer_size: The buffer size to use when shuffling the dataset. As the dataset is</span>
<span class="sd">            streamed, this just pre-downloads at least `buffer_size` items and then shuffles</span>
<span class="sd">            just that buffer. Note that the generated activations should also be shuffled before</span>
<span class="sd">            training the sparse autoencoder, so a large buffer may not be strictly necessary</span>
<span class="sd">            here. Note also that this is the number of items in the dataset (e.g. number of</span>
<span class="sd">            prompts) and is typically significantly less than the number of tokenized prompts</span>
<span class="sd">            once the preprocessing function has been applied.</span>
<span class="sd">        preprocess_batch_size: The batch size to use just for preprocessing the dataset (e.g.</span>
<span class="sd">            tokenizing prompts).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">context_size</span> <span class="o">=</span> <span class="n">context_size</span>

    <span class="c1"># Load the dataset</span>
    <span class="n">dataset</span><span class="p">:</span> <span class="n">IterableDataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="n">dataset_path</span><span class="p">,</span> <span class="n">streaming</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="n">dataset_split</span><span class="p">)</span>  <span class="c1"># type: ignore</span>

    <span class="c1"># Setup preprocessing</span>
    <span class="n">existing_columns</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">dataset</span><span class="p">))</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
    <span class="n">mapped_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">preprocess</span><span class="p">,</span>
        <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="n">preprocess_batch_size</span><span class="p">,</span>
        <span class="n">fn_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;context_size&quot;</span><span class="p">:</span> <span class="n">context_size</span><span class="p">},</span>
        <span class="n">remove_columns</span><span class="o">=</span><span class="n">existing_columns</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Setup approximate shuffling. As the dataset is streamed, this just pre-downloads at least</span>
    <span class="c1"># `buffer_size` items and then shuffles just that buffer.</span>
    <span class="c1"># https://huggingface.co/docs/datasets/v2.14.5/stream#shuffle</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span> <span class="o">=</span> <span class="n">mapped_dataset</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">buffer_size</span><span class="o">=</span><span class="n">buffer_size</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.source_data.abstract_dataset.SourceDataset.__iter__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__iter__</span><span class="p">()</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Iterate Dunder Method.</p>
<p>Enables direct access to :attr:<code>dataset</code> with e.g. <code>for</code> loops.</p>

          <details class="quote">
            <summary> <code>sparse_autoencoder/source_data/abstract_dataset.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@final</span>
<span class="k">def</span> <span class="fm">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>  <span class="c1"># noqa: ANN401</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Iterate Dunder Method.</span>

<span class="sd">    Enables direct access to :attr:`dataset` with e.g. `for` loops.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="fm">__iter__</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.source_data.abstract_dataset.SourceDataset.__next__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__next__</span><span class="p">()</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Next Dunder Method.</p>
<p>Enables direct access to :attr:<code>dataset</code> with e.g. <code>next</code> calls.</p>

          <details class="quote">
            <summary> <code>sparse_autoencoder/source_data/abstract_dataset.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@final</span>
<span class="k">def</span> <span class="fm">__next__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>  <span class="c1"># noqa: ANN401</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Next Dunder Method.</span>

<span class="sd">    Enables direct access to :attr:`dataset` with e.g. `next` calls.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="bp">self</span><span class="p">))</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.source_data.abstract_dataset.SourceDataset.get_dataloader" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">get_dataloader</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Get a PyTorch DataLoader.</p>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>batch_size</code></b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>The batch size to use.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
    <th class="field-name">Returns:</th>
    <td class="field-body">
      <ul class="first simple">
          <li>
                <code><span title="torch.utils.data.DataLoader">DataLoader</span>[<a class="autorefs autorefs-internal" title="sparse_autoencoder.source_data.abstract_dataset.TorchTokenizedPrompts" href="#sparse_autoencoder.source_data.abstract_dataset.TorchTokenizedPrompts">TorchTokenizedPrompts</a>]</code>
            –
            <div class="doc-md-description">
              <p>PyTorch DataLoader.</p>
            </div>
          </li>
      </ul>
    </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary> <code>sparse_autoencoder/source_data/abstract_dataset.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@final</span>
<span class="k">def</span> <span class="nf">get_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">DataLoader</span><span class="p">[</span><span class="n">TorchTokenizedPrompts</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Get a PyTorch DataLoader.</span>

<span class="sd">    Args:</span>
<span class="sd">        batch_size: The batch size to use.</span>

<span class="sd">    Returns:</span>
<span class="sd">        PyTorch DataLoader.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch_dataset</span><span class="p">:</span> <span class="n">TorchDataset</span><span class="p">[</span><span class="n">TorchTokenizedPrompts</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">with_format</span><span class="p">(</span><span class="s2">&quot;torch&quot;</span><span class="p">)</span>  <span class="c1"># type: ignore</span>

    <span class="k">return</span> <span class="n">DataLoader</span><span class="p">[</span><span class="n">TorchTokenizedPrompts</span><span class="p">](</span>
        <span class="n">torch_dataset</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="c1"># Shuffle is most efficiently done with the `shuffle` method on the dataset itself, not</span>
        <span class="c1"># here.</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.source_data.abstract_dataset.SourceDataset.preprocess" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">preprocess</span><span class="p">(</span><span class="n">source_batch</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">context_size</span><span class="p">)</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-abstractmethod"><code>abstractmethod</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>Preprocess function.</p>
<p>Takes a <code>preprocess_batch_size</code> ($m$) batch of source data (which may e.g. include string
prompts), and returns a dict with a single key of <code>input_ids</code> and a value of an arbitrary
length list ($n$) of tokenized prompts. Note that $m$ does not have to be equal to $n$.</p>
<p>Applied to the dataset with the <a href="https://huggingface.co/docs/datasets/v2.14.5/en/package_reference/main_classes#datasets.Dataset.map">Hugging Face
Dataset</a>
<code>map</code> function.</p>

<details class="warning" open>
  <summary>Warning</summary>
  <p>The returned tokenized prompts should not have any padding tokens (apart from an
optional single first padding token).</p>
</details>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>source_batch</code></b>
                  (<code><a class="autorefs autorefs-internal" title="sparse_autoencoder.source_data.abstract_dataset.HuggingFaceDatasetItem" href="#sparse_autoencoder.source_data.abstract_dataset.HuggingFaceDatasetItem">HuggingFaceDatasetItem</a></code>)
              –
              <div class="doc-md-description">
                <p>A batch of source data. For example, with The Pile dataset this would be a
dict including the key "text" with a value of a list of strings (not yet tokenized).</p>
              </div>
            </li>
            <li>
              <b><code>context_size</code></b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>The context size to use when returning a list of tokenized prompts.
<em>Towards Monosemanticity: Decomposing Language Models With Dictionary Learning</em> used
a context size of 250.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary> <code>sparse_autoencoder/source_data/abstract_dataset.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span>
<span class="normal">85</span>
<span class="normal">86</span>
<span class="normal">87</span>
<span class="normal">88</span>
<span class="normal">89</span>
<span class="normal">90</span>
<span class="normal">91</span>
<span class="normal">92</span>
<span class="normal">93</span>
<span class="normal">94</span>
<span class="normal">95</span>
<span class="normal">96</span>
<span class="normal">97</span>
<span class="normal">98</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@abstractmethod</span>
<span class="k">def</span> <span class="nf">preprocess</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">source_batch</span><span class="p">:</span> <span class="n">HuggingFaceDatasetItem</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">context_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TokenizedPrompts</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Preprocess function.</span>

<span class="sd">    Takes a `preprocess_batch_size` ($m$) batch of source data (which may e.g. include string</span>
<span class="sd">    prompts), and returns a dict with a single key of `input_ids` and a value of an arbitrary</span>
<span class="sd">    length list ($n$) of tokenized prompts. Note that $m$ does not have to be equal to $n$.</span>

<span class="sd">    Applied to the dataset with the [Hugging Face</span>
<span class="sd">    Dataset](https://huggingface.co/docs/datasets/v2.14.5/en/package_reference/main_classes#datasets.Dataset.map)</span>
<span class="sd">    `map` function.</span>

<span class="sd">    Warning:</span>
<span class="sd">        The returned tokenized prompts should not have any padding tokens (apart from an</span>
<span class="sd">        optional single first padding token).</span>

<span class="sd">    Args:</span>
<span class="sd">        source_batch: A batch of source data. For example, with The Pile dataset this would be a</span>
<span class="sd">            dict including the key &quot;text&quot; with a value of a list of strings (not yet tokenized).</span>
<span class="sd">        context_size: The context size to use when returning a list of tokenized prompts.</span>
<span class="sd">            *Towards Monosemanticity: Decomposing Language Models With Dictionary Learning* used</span>
<span class="sd">            a context size of 250.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">




<h2 id="sparse_autoencoder.source_data.abstract_dataset.TokenizedPrompts" class="doc doc-heading">
          <code>TokenizedPrompts</code>


</h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><span title="typing.TypedDict">TypedDict</span></code></p>

  
      <p>Tokenized prompts.</p>

            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/source_data/abstract_dataset.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">TokenizedPrompts</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Tokenized prompts.&quot;&quot;&quot;</span>

    <span class="n">input_ids</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">TokenizedPrompt</span><span class="p">]</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">











  </div>

  </div>

</div>

<div class="doc doc-object doc-class">




<h2 id="sparse_autoencoder.source_data.abstract_dataset.TorchTokenizedPrompts" class="doc doc-heading">
          <code>TorchTokenizedPrompts</code>


</h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><span title="typing.TypedDict">TypedDict</span></code></p>

  
      <p>Tokenized prompts prepared for PyTorch.</p>

            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/source_data/abstract_dataset.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">TorchTokenizedPrompts</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Tokenized prompts prepared for PyTorch.&quot;&quot;&quot;</span>

    <span class="n">input_ids</span><span class="p">:</span> <span class="n">BatchTokenizedPrompts</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">











  </div>

  </div>

</div>




  </div>

  </div>

</div>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../" class="btn btn-neutral float-left" title="Index"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../c4_pre_tokenized/" class="btn btn-neutral float-right" title="c4_pre_tokenized">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../c4_pre_tokenized/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../../../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "../../..";</script>
    <script src="../../../js/theme_extra.js"></script>
    <script src="../../../js/theme.js"></script>
      <script src="../../../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
